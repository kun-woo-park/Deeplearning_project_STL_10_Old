{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import torch.distributed as dist\n",
    "import math\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import Type, Any, Callable, Union, List, Optional\n",
    "from torch import Tensor\n",
    "from torch.hub import load_state_dict_from_url\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.data.sampler import BatchSampler\n",
    "from collections.abc import Mapping, Sequence\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path_path= f\"../data/split_train/train\"\n",
    "test_path_path= f\"../data/split_train/val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "num_gpus=4\n",
    "num_workers=64\n",
    "lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.507, 0.487, 0.441],\n",
    "                                     std=[0.267, 0.256, 0.276])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder(\n",
    "        train_path_path,\n",
    "        transforms.Compose([\n",
    "        transforms.RandomRotation(degrees=45),\n",
    "            \n",
    "        transforms.RandomResizedCrop(70),\n",
    "        transforms.ColorJitter(.3,.3,.3,.3),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "for _ in range (19):\n",
    "    aug_data = datasets.ImageFolder(\n",
    "        train_path_path,\n",
    "        transforms.Compose([\n",
    "        transforms.RandomRotation(degrees=45),\n",
    "            \n",
    "        transforms.RandomResizedCrop(70),\n",
    "        transforms.ColorJitter(.3,.3,.3,.3),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "    dataset=torch.utils.data.ConcatDataset([dataset,aug_data])\n",
    "valset = datasets.ImageFolder(\n",
    "        test_path_path,\n",
    "        transforms.Compose([\n",
    "        transforms.RandomRotation(degrees=45),\n",
    "            \n",
    "        transforms.RandomResizedCrop(70),\n",
    "        transforms.ColorJitter(.3,.3,.3,.3),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "for _ in range (19):\n",
    "    aug_data = datasets.ImageFolder(\n",
    "        test_path_path,\n",
    "        transforms.Compose([\n",
    "        transforms.RandomRotation(degrees=45),\n",
    "            \n",
    "        transforms.RandomResizedCrop(70),\n",
    "        transforms.ColorJitter(.3,.3,.3,.3),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "    valset=torch.utils.data.ConcatDataset([valset,aug_data])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=True,\n",
    "        num_workers=8, pin_memory=True,drop_last=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        valset, batch_size=batch_size, shuffle=True,\n",
    "        num_workers=8, pin_memory=True,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
    "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
    "    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n",
    "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
    "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
    "\n",
    "    expansion: int = 4\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: Optional[nn.Module] = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
    "    ) -> None:\n",
    "        super(Bottleneck, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        width = int(planes * (base_width / 64.)) * groups\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.LeakyReLU(0.1)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        block: Type[Union[Bottleneck]],\n",
    "        layers: List[int],\n",
    "        num_classes: int = 10,\n",
    "        zero_init_residual: bool = False,\n",
    "        groups: int = 1,\n",
    "        width_per_group: int = 64,\n",
    "        replace_stride_with_dilation: Optional[List[bool]] = None,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
    "    ) -> None:\n",
    "        super(ResNet, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
    "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=4, stride=1,\n",
    "                               bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.LeakyReLU(0.1)\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.layer1 = self._make_layer(block, 30, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 60, layers[1], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 96, layers[2], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[1])\n",
    "        \n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(96 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]\n",
    "\n",
    "    def _make_layer(self, block: Type[Union[Bottleneck]], planes: int, blocks: int,\n",
    "                    stride: int = 1, dilate: bool = False) -> nn.Sequential:\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "                            self.base_width, previous_dilation, norm_layer))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation,\n",
    "                                norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x,1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self._forward_impl(x)\n",
    "\n",
    "\n",
    "def _resnet(\n",
    "    arch: str,\n",
    "    block: Type[Union[Bottleneck]],\n",
    "    layers: List[int],\n",
    "    pretrained: bool,\n",
    "    progress: bool,\n",
    "    **kwargs: Any\n",
    ") -> ResNet:\n",
    "    model = ResNet(block, layers, **kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
    "                                              progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "def _resnext(arch, block, layers, pretrained, progress, **kwargs):\n",
    "    model = ResNet(block, layers, **kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = load_state_dict_from_url(model_urls[arch], progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model(pretrained: bool = False, progress: bool = True, **kwargs):\n",
    "    \n",
    "    kwargs['groups'] = 1\n",
    "    kwargs['width_per_group'] = 64\n",
    "    return _resnext('resnext', Bottleneck, [4, 9, 8], pretrained, progress, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.1, dim=-1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "model = model.cuda()\n",
    "criterion = LabelSmoothingLoss(classes=10).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print (name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 93, 93]           3,072\n",
      "       BatchNorm2d-2           [-1, 64, 93, 93]             128\n",
      "         LeakyReLU-3           [-1, 64, 93, 93]               0\n",
      "         MaxPool2d-4           [-1, 64, 46, 46]               0\n",
      "            Conv2d-5           [-1, 30, 46, 46]           1,920\n",
      "       BatchNorm2d-6           [-1, 30, 46, 46]              60\n",
      "         LeakyReLU-7           [-1, 30, 46, 46]               0\n",
      "            Conv2d-8           [-1, 30, 46, 46]           8,100\n",
      "       BatchNorm2d-9           [-1, 30, 46, 46]              60\n",
      "        LeakyReLU-10           [-1, 30, 46, 46]               0\n",
      "           Conv2d-11          [-1, 120, 46, 46]           3,600\n",
      "      BatchNorm2d-12          [-1, 120, 46, 46]             240\n",
      "           Conv2d-13          [-1, 120, 46, 46]           7,680\n",
      "      BatchNorm2d-14          [-1, 120, 46, 46]             240\n",
      "        LeakyReLU-15          [-1, 120, 46, 46]               0\n",
      "       Bottleneck-16          [-1, 120, 46, 46]               0\n",
      "           Conv2d-17           [-1, 30, 46, 46]           3,600\n",
      "      BatchNorm2d-18           [-1, 30, 46, 46]              60\n",
      "        LeakyReLU-19           [-1, 30, 46, 46]               0\n",
      "           Conv2d-20           [-1, 30, 46, 46]           8,100\n",
      "      BatchNorm2d-21           [-1, 30, 46, 46]              60\n",
      "        LeakyReLU-22           [-1, 30, 46, 46]               0\n",
      "           Conv2d-23          [-1, 120, 46, 46]           3,600\n",
      "      BatchNorm2d-24          [-1, 120, 46, 46]             240\n",
      "        LeakyReLU-25          [-1, 120, 46, 46]               0\n",
      "       Bottleneck-26          [-1, 120, 46, 46]               0\n",
      "           Conv2d-27           [-1, 30, 46, 46]           3,600\n",
      "      BatchNorm2d-28           [-1, 30, 46, 46]              60\n",
      "        LeakyReLU-29           [-1, 30, 46, 46]               0\n",
      "           Conv2d-30           [-1, 30, 46, 46]           8,100\n",
      "      BatchNorm2d-31           [-1, 30, 46, 46]              60\n",
      "        LeakyReLU-32           [-1, 30, 46, 46]               0\n",
      "           Conv2d-33          [-1, 120, 46, 46]           3,600\n",
      "      BatchNorm2d-34          [-1, 120, 46, 46]             240\n",
      "        LeakyReLU-35          [-1, 120, 46, 46]               0\n",
      "       Bottleneck-36          [-1, 120, 46, 46]               0\n",
      "           Conv2d-37           [-1, 30, 46, 46]           3,600\n",
      "      BatchNorm2d-38           [-1, 30, 46, 46]              60\n",
      "        LeakyReLU-39           [-1, 30, 46, 46]               0\n",
      "           Conv2d-40           [-1, 30, 46, 46]           8,100\n",
      "      BatchNorm2d-41           [-1, 30, 46, 46]              60\n",
      "        LeakyReLU-42           [-1, 30, 46, 46]               0\n",
      "           Conv2d-43          [-1, 120, 46, 46]           3,600\n",
      "      BatchNorm2d-44          [-1, 120, 46, 46]             240\n",
      "        LeakyReLU-45          [-1, 120, 46, 46]               0\n",
      "       Bottleneck-46          [-1, 120, 46, 46]               0\n",
      "           Conv2d-47           [-1, 60, 46, 46]           7,200\n",
      "      BatchNorm2d-48           [-1, 60, 46, 46]             120\n",
      "        LeakyReLU-49           [-1, 60, 46, 46]               0\n",
      "           Conv2d-50           [-1, 60, 23, 23]          32,400\n",
      "      BatchNorm2d-51           [-1, 60, 23, 23]             120\n",
      "        LeakyReLU-52           [-1, 60, 23, 23]               0\n",
      "           Conv2d-53          [-1, 240, 23, 23]          14,400\n",
      "      BatchNorm2d-54          [-1, 240, 23, 23]             480\n",
      "           Conv2d-55          [-1, 240, 23, 23]          28,800\n",
      "      BatchNorm2d-56          [-1, 240, 23, 23]             480\n",
      "        LeakyReLU-57          [-1, 240, 23, 23]               0\n",
      "       Bottleneck-58          [-1, 240, 23, 23]               0\n",
      "           Conv2d-59           [-1, 60, 23, 23]          14,400\n",
      "      BatchNorm2d-60           [-1, 60, 23, 23]             120\n",
      "        LeakyReLU-61           [-1, 60, 23, 23]               0\n",
      "           Conv2d-62           [-1, 60, 23, 23]          32,400\n",
      "      BatchNorm2d-63           [-1, 60, 23, 23]             120\n",
      "        LeakyReLU-64           [-1, 60, 23, 23]               0\n",
      "           Conv2d-65          [-1, 240, 23, 23]          14,400\n",
      "      BatchNorm2d-66          [-1, 240, 23, 23]             480\n",
      "        LeakyReLU-67          [-1, 240, 23, 23]               0\n",
      "       Bottleneck-68          [-1, 240, 23, 23]               0\n",
      "           Conv2d-69           [-1, 60, 23, 23]          14,400\n",
      "      BatchNorm2d-70           [-1, 60, 23, 23]             120\n",
      "        LeakyReLU-71           [-1, 60, 23, 23]               0\n",
      "           Conv2d-72           [-1, 60, 23, 23]          32,400\n",
      "      BatchNorm2d-73           [-1, 60, 23, 23]             120\n",
      "        LeakyReLU-74           [-1, 60, 23, 23]               0\n",
      "           Conv2d-75          [-1, 240, 23, 23]          14,400\n",
      "      BatchNorm2d-76          [-1, 240, 23, 23]             480\n",
      "        LeakyReLU-77          [-1, 240, 23, 23]               0\n",
      "       Bottleneck-78          [-1, 240, 23, 23]               0\n",
      "           Conv2d-79           [-1, 60, 23, 23]          14,400\n",
      "      BatchNorm2d-80           [-1, 60, 23, 23]             120\n",
      "        LeakyReLU-81           [-1, 60, 23, 23]               0\n",
      "           Conv2d-82           [-1, 60, 23, 23]          32,400\n",
      "      BatchNorm2d-83           [-1, 60, 23, 23]             120\n",
      "        LeakyReLU-84           [-1, 60, 23, 23]               0\n",
      "           Conv2d-85          [-1, 240, 23, 23]          14,400\n",
      "      BatchNorm2d-86          [-1, 240, 23, 23]             480\n",
      "        LeakyReLU-87          [-1, 240, 23, 23]               0\n",
      "       Bottleneck-88          [-1, 240, 23, 23]               0\n",
      "           Conv2d-89           [-1, 60, 23, 23]          14,400\n",
      "      BatchNorm2d-90           [-1, 60, 23, 23]             120\n",
      "        LeakyReLU-91           [-1, 60, 23, 23]               0\n",
      "           Conv2d-92           [-1, 60, 23, 23]          32,400\n",
      "      BatchNorm2d-93           [-1, 60, 23, 23]             120\n",
      "        LeakyReLU-94           [-1, 60, 23, 23]               0\n",
      "           Conv2d-95          [-1, 240, 23, 23]          14,400\n",
      "      BatchNorm2d-96          [-1, 240, 23, 23]             480\n",
      "        LeakyReLU-97          [-1, 240, 23, 23]               0\n",
      "       Bottleneck-98          [-1, 240, 23, 23]               0\n",
      "           Conv2d-99           [-1, 60, 23, 23]          14,400\n",
      "     BatchNorm2d-100           [-1, 60, 23, 23]             120\n",
      "       LeakyReLU-101           [-1, 60, 23, 23]               0\n",
      "          Conv2d-102           [-1, 60, 23, 23]          32,400\n",
      "     BatchNorm2d-103           [-1, 60, 23, 23]             120\n",
      "       LeakyReLU-104           [-1, 60, 23, 23]               0\n",
      "          Conv2d-105          [-1, 240, 23, 23]          14,400\n",
      "     BatchNorm2d-106          [-1, 240, 23, 23]             480\n",
      "       LeakyReLU-107          [-1, 240, 23, 23]               0\n",
      "      Bottleneck-108          [-1, 240, 23, 23]               0\n",
      "          Conv2d-109           [-1, 60, 23, 23]          14,400\n",
      "     BatchNorm2d-110           [-1, 60, 23, 23]             120\n",
      "       LeakyReLU-111           [-1, 60, 23, 23]               0\n",
      "          Conv2d-112           [-1, 60, 23, 23]          32,400\n",
      "     BatchNorm2d-113           [-1, 60, 23, 23]             120\n",
      "       LeakyReLU-114           [-1, 60, 23, 23]               0\n",
      "          Conv2d-115          [-1, 240, 23, 23]          14,400\n",
      "     BatchNorm2d-116          [-1, 240, 23, 23]             480\n",
      "       LeakyReLU-117          [-1, 240, 23, 23]               0\n",
      "      Bottleneck-118          [-1, 240, 23, 23]               0\n",
      "          Conv2d-119           [-1, 60, 23, 23]          14,400\n",
      "     BatchNorm2d-120           [-1, 60, 23, 23]             120\n",
      "       LeakyReLU-121           [-1, 60, 23, 23]               0\n",
      "          Conv2d-122           [-1, 60, 23, 23]          32,400\n",
      "     BatchNorm2d-123           [-1, 60, 23, 23]             120\n",
      "       LeakyReLU-124           [-1, 60, 23, 23]               0\n",
      "          Conv2d-125          [-1, 240, 23, 23]          14,400\n",
      "     BatchNorm2d-126          [-1, 240, 23, 23]             480\n",
      "       LeakyReLU-127          [-1, 240, 23, 23]               0\n",
      "      Bottleneck-128          [-1, 240, 23, 23]               0\n",
      "          Conv2d-129           [-1, 60, 23, 23]          14,400\n",
      "     BatchNorm2d-130           [-1, 60, 23, 23]             120\n",
      "       LeakyReLU-131           [-1, 60, 23, 23]               0\n",
      "          Conv2d-132           [-1, 60, 23, 23]          32,400\n",
      "     BatchNorm2d-133           [-1, 60, 23, 23]             120\n",
      "       LeakyReLU-134           [-1, 60, 23, 23]               0\n",
      "          Conv2d-135          [-1, 240, 23, 23]          14,400\n",
      "     BatchNorm2d-136          [-1, 240, 23, 23]             480\n",
      "       LeakyReLU-137          [-1, 240, 23, 23]               0\n",
      "      Bottleneck-138          [-1, 240, 23, 23]               0\n",
      "          Conv2d-139           [-1, 96, 23, 23]          23,040\n",
      "     BatchNorm2d-140           [-1, 96, 23, 23]             192\n",
      "       LeakyReLU-141           [-1, 96, 23, 23]               0\n",
      "          Conv2d-142           [-1, 96, 12, 12]          82,944\n",
      "     BatchNorm2d-143           [-1, 96, 12, 12]             192\n",
      "       LeakyReLU-144           [-1, 96, 12, 12]               0\n",
      "          Conv2d-145          [-1, 384, 12, 12]          36,864\n",
      "     BatchNorm2d-146          [-1, 384, 12, 12]             768\n",
      "          Conv2d-147          [-1, 384, 12, 12]          92,160\n",
      "     BatchNorm2d-148          [-1, 384, 12, 12]             768\n",
      "       LeakyReLU-149          [-1, 384, 12, 12]               0\n",
      "      Bottleneck-150          [-1, 384, 12, 12]               0\n",
      "          Conv2d-151           [-1, 96, 12, 12]          36,864\n",
      "     BatchNorm2d-152           [-1, 96, 12, 12]             192\n",
      "       LeakyReLU-153           [-1, 96, 12, 12]               0\n",
      "          Conv2d-154           [-1, 96, 12, 12]          82,944\n",
      "     BatchNorm2d-155           [-1, 96, 12, 12]             192\n",
      "       LeakyReLU-156           [-1, 96, 12, 12]               0\n",
      "          Conv2d-157          [-1, 384, 12, 12]          36,864\n",
      "     BatchNorm2d-158          [-1, 384, 12, 12]             768\n",
      "       LeakyReLU-159          [-1, 384, 12, 12]               0\n",
      "      Bottleneck-160          [-1, 384, 12, 12]               0\n",
      "          Conv2d-161           [-1, 96, 12, 12]          36,864\n",
      "     BatchNorm2d-162           [-1, 96, 12, 12]             192\n",
      "       LeakyReLU-163           [-1, 96, 12, 12]               0\n",
      "          Conv2d-164           [-1, 96, 12, 12]          82,944\n",
      "     BatchNorm2d-165           [-1, 96, 12, 12]             192\n",
      "       LeakyReLU-166           [-1, 96, 12, 12]               0\n",
      "          Conv2d-167          [-1, 384, 12, 12]          36,864\n",
      "     BatchNorm2d-168          [-1, 384, 12, 12]             768\n",
      "       LeakyReLU-169          [-1, 384, 12, 12]               0\n",
      "      Bottleneck-170          [-1, 384, 12, 12]               0\n",
      "          Conv2d-171           [-1, 96, 12, 12]          36,864\n",
      "     BatchNorm2d-172           [-1, 96, 12, 12]             192\n",
      "       LeakyReLU-173           [-1, 96, 12, 12]               0\n",
      "          Conv2d-174           [-1, 96, 12, 12]          82,944\n",
      "     BatchNorm2d-175           [-1, 96, 12, 12]             192\n",
      "       LeakyReLU-176           [-1, 96, 12, 12]               0\n",
      "          Conv2d-177          [-1, 384, 12, 12]          36,864\n",
      "     BatchNorm2d-178          [-1, 384, 12, 12]             768\n",
      "       LeakyReLU-179          [-1, 384, 12, 12]               0\n",
      "      Bottleneck-180          [-1, 384, 12, 12]               0\n",
      "          Conv2d-181           [-1, 96, 12, 12]          36,864\n",
      "     BatchNorm2d-182           [-1, 96, 12, 12]             192\n",
      "       LeakyReLU-183           [-1, 96, 12, 12]               0\n",
      "          Conv2d-184           [-1, 96, 12, 12]          82,944\n",
      "     BatchNorm2d-185           [-1, 96, 12, 12]             192\n",
      "       LeakyReLU-186           [-1, 96, 12, 12]               0\n",
      "          Conv2d-187          [-1, 384, 12, 12]          36,864\n",
      "     BatchNorm2d-188          [-1, 384, 12, 12]             768\n",
      "       LeakyReLU-189          [-1, 384, 12, 12]               0\n",
      "      Bottleneck-190          [-1, 384, 12, 12]               0\n",
      "          Conv2d-191           [-1, 96, 12, 12]          36,864\n",
      "     BatchNorm2d-192           [-1, 96, 12, 12]             192\n",
      "       LeakyReLU-193           [-1, 96, 12, 12]               0\n",
      "          Conv2d-194           [-1, 96, 12, 12]          82,944\n",
      "     BatchNorm2d-195           [-1, 96, 12, 12]             192\n",
      "       LeakyReLU-196           [-1, 96, 12, 12]               0\n",
      "          Conv2d-197          [-1, 384, 12, 12]          36,864\n",
      "     BatchNorm2d-198          [-1, 384, 12, 12]             768\n",
      "       LeakyReLU-199          [-1, 384, 12, 12]               0\n",
      "      Bottleneck-200          [-1, 384, 12, 12]               0\n",
      "          Conv2d-201           [-1, 96, 12, 12]          36,864\n",
      "     BatchNorm2d-202           [-1, 96, 12, 12]             192\n",
      "       LeakyReLU-203           [-1, 96, 12, 12]               0\n",
      "          Conv2d-204           [-1, 96, 12, 12]          82,944\n",
      "     BatchNorm2d-205           [-1, 96, 12, 12]             192\n",
      "       LeakyReLU-206           [-1, 96, 12, 12]               0\n",
      "          Conv2d-207          [-1, 384, 12, 12]          36,864\n",
      "     BatchNorm2d-208          [-1, 384, 12, 12]             768\n",
      "       LeakyReLU-209          [-1, 384, 12, 12]               0\n",
      "      Bottleneck-210          [-1, 384, 12, 12]               0\n",
      "          Conv2d-211           [-1, 96, 12, 12]          36,864\n",
      "     BatchNorm2d-212           [-1, 96, 12, 12]             192\n",
      "       LeakyReLU-213           [-1, 96, 12, 12]               0\n",
      "          Conv2d-214           [-1, 96, 12, 12]          82,944\n",
      "     BatchNorm2d-215           [-1, 96, 12, 12]             192\n",
      "       LeakyReLU-216           [-1, 96, 12, 12]               0\n",
      "          Conv2d-217          [-1, 384, 12, 12]          36,864\n",
      "     BatchNorm2d-218          [-1, 384, 12, 12]             768\n",
      "       LeakyReLU-219          [-1, 384, 12, 12]               0\n",
      "      Bottleneck-220          [-1, 384, 12, 12]               0\n",
      "AdaptiveAvgPool2d-221            [-1, 384, 1, 1]               0\n",
      "          Linear-222                   [-1, 10]           3,850\n",
      "================================================================\n",
      "Total params: 1,996,986\n",
      "Trainable params: 1,996,986\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.11\n",
      "Forward/backward pass size (MB): 132.52\n",
      "Params size (MB): 7.62\n",
      "Estimated Total Size (MB): 140.24\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model,(3,96,96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_path=\"/home/jupyter-deeplearning/res_model/\"\n",
    "trn_loss_list = []\n",
    "val_loss_list = []\n",
    "val_acc_list = []\n",
    "total_epoch=200\n",
    "model_char=\"2.0\"\n",
    "model_name=\"\"\n",
    "patience=10\n",
    "start_early_stop_check=0\n",
    "saving_start_epoch=10\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    trn_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        if torch.cuda.is_available():\n",
    "            inputs=inputs.cuda()\n",
    "            labels=labels.cuda()\n",
    "        # grad init\n",
    "        optimizer.zero_grad()\n",
    "        # forward propagation\n",
    "        output= model(inputs)\n",
    "        # calculate loss\n",
    "        loss=criterion(output, labels)\n",
    "        # back propagation \n",
    "        loss.backward()\n",
    "        # weight update\n",
    "        optimizer.step()\n",
    "        \n",
    "        # trn_loss summary\n",
    "        trn_loss += loss.item()\n",
    "        # del (memory issue)\n",
    "        del loss\n",
    "        del output\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        cor_match = 0\n",
    "        for j, val in enumerate(test_loader):\n",
    "            val_x, val_label = val\n",
    "            if torch.cuda.is_available():\n",
    "                val_x = val_x.cuda()\n",
    "                val_label =val_label.cuda()\n",
    "            val_output = model(val_x)\n",
    "            v_loss = criterion(val_output, val_label)\n",
    "            val_loss += v_loss\n",
    "            _, predicted=torch.max(val_output,1)\n",
    "            cor_match+=np.count_nonzero(predicted.cpu().detach()==val_label.cpu().detach())\n",
    "    del val_output\n",
    "    del v_loss\n",
    "    del predicted\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    \n",
    "    \n",
    "    trn_loss_list.append(trn_loss/len(train_loader))\n",
    "    val_loss_list.append(val_loss/len(test_loader))\n",
    "    val_acc=cor_match/(len(test_loader)*batch_size)\n",
    "    val_acc_list.append(val_acc)\n",
    "    now = time.localtime()\n",
    "    print (\"%04d/%02d/%02d %02d:%02d:%02d\" % (now.tm_year, now.tm_mon, now.tm_mday, now.tm_hour, now.tm_min, now.tm_sec))\n",
    "\n",
    "    print(\"epoch: {}/{} | trn loss: {:.4f} | val loss: {:.4f} | val accuracy: {:.4f}% \\n\".format(\n",
    "                epoch+1, total_epoch, trn_loss / len(train_loader), val_loss / len(test_loader), val_acc*100\n",
    "            ))\n",
    "    \n",
    "    \n",
    "    if epoch+1>2:\n",
    "        if val_loss_list[-1]>val_loss_list[-2]:\n",
    "            start_early_stop_check=1\n",
    "    else:\n",
    "        val_loss_min=val_loss_list[-1]\n",
    "        \n",
    "    if start_early_stop_check:\n",
    "        early_stop_temp=val_loss_list[-patience:]\n",
    "        if all(early_stop_temp[i]<early_stop_temp[i+1] for i in range (len(early_stop_temp)-1)):\n",
    "            print(\"Early stop!\")\n",
    "            break\n",
    "            \n",
    "    if epoch+1>saving_start_epoch:\n",
    "        if val_loss_list[-1]<val_loss_min:\n",
    "            if os.path.isfile(model_name):\n",
    "                os.remove(model_name)\n",
    "            val_loss_min=val_loss_list[-1]\n",
    "            model_name=saving_path+\"Custom_model_\"+model_char+\"_{:.3f}\".format(val_loss_min)\n",
    "            torch.save(model, model_name)\n",
    "            print(\"Model replaced and saved as \",model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder(\n",
    "        train_path_path,\n",
    "        transforms.Compose([\n",
    "        transforms.RandomRotation(degrees=45),\n",
    "        transforms.Resize(120),    \n",
    "        transforms.ColorJitter(.3,.3,.3,.3),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "for _ in range (19):\n",
    "    aug_data = datasets.ImageFolder(\n",
    "        train_path_path,\n",
    "        transforms.Compose([\n",
    "        transforms.RandomRotation(degrees=45),\n",
    "        transforms.Resize(120),    \n",
    "        transforms.ColorJitter(.3,.3,.3,.3),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "    dataset=torch.utils.data.ConcatDataset([dataset,aug_data])\n",
    "valset = datasets.ImageFolder(\n",
    "        test_path_path,\n",
    "        transforms.Compose([\n",
    "        transforms.RandomRotation(degrees=45),\n",
    "        transforms.Resize(120),\n",
    "        transforms.ColorJitter(.3,.3,.3,.3),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "for _ in range (19):\n",
    "    aug_data = datasets.ImageFolder(\n",
    "        test_path_path,\n",
    "        transforms.Compose([\n",
    "        transforms.RandomRotation(degrees=45),\n",
    "        transforms.Resize(120),    \n",
    "        transforms.ColorJitter(.3,.3,.3,.3),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "    valset=torch.utils.data.ConcatDataset([valset,aug_data])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=True,\n",
    "        num_workers=8, pin_memory=True,drop_last=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        valset, batch_size=batch_size, shuffle=True,\n",
    "        num_workers=8, pin_memory=True,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fine tuning\n",
    "\n",
    "total_epoch=80\n",
    "model_char=\"2.0\"\n",
    "model_name=\"\"\n",
    "patience=10\n",
    "start_early_stop_check=0\n",
    "saving_start_epoch=10\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    trn_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        if torch.cuda.is_available():\n",
    "            inputs=inputs.cuda()\n",
    "            labels=labels.cuda()\n",
    "        # grad init\n",
    "        optimizer.zero_grad()\n",
    "        # forward propagation\n",
    "        output= model(inputs)\n",
    "        # calculate loss\n",
    "        loss=criterion(output, labels)\n",
    "        # back propagation \n",
    "        loss.backward()\n",
    "        # weight update\n",
    "        optimizer.step()\n",
    "        \n",
    "        # trn_loss summary\n",
    "        trn_loss += loss.item()\n",
    "        # del (memory issue)\n",
    "        del loss\n",
    "        del output\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        cor_match = 0\n",
    "        for j, val in enumerate(test_loader):\n",
    "            val_x, val_label = val\n",
    "            if torch.cuda.is_available():\n",
    "                val_x = val_x.cuda()\n",
    "                val_label =val_label.cuda()\n",
    "            val_output = model(val_x)\n",
    "            v_loss = criterion(val_output, val_label)\n",
    "            val_loss += v_loss\n",
    "            _, predicted=torch.max(val_output,1)\n",
    "            cor_match+=np.count_nonzero(predicted.cpu().detach()==val_label.cpu().detach())\n",
    "    del val_output\n",
    "    del v_loss\n",
    "    del predicted\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    \n",
    "    \n",
    "    trn_loss_list.append(trn_loss/len(train_loader))\n",
    "    val_loss_list.append(val_loss/len(test_loader))\n",
    "    val_acc=cor_match/(len(test_loader)*batch_size)\n",
    "    val_acc_list.append(val_acc)\n",
    "    now = time.localtime()\n",
    "    print (\"%04d/%02d/%02d %02d:%02d:%02d\" % (now.tm_year, now.tm_mon, now.tm_mday, now.tm_hour, now.tm_min, now.tm_sec))\n",
    "\n",
    "    print(\"epoch: {}/{} | trn loss: {:.4f} | val loss: {:.4f} | val accuracy: {:.4f}% \\n\".format(\n",
    "                epoch+1, total_epoch, trn_loss / len(train_loader), val_loss / len(test_loader), val_acc*100\n",
    "            ))\n",
    "    \n",
    "    \n",
    "    if epoch+1>2:\n",
    "        if val_loss_list[-1]>val_loss_list[-2]:\n",
    "            start_early_stop_check=1\n",
    "    else:\n",
    "        val_loss_min=val_loss_list[-1]\n",
    "        \n",
    "    if start_early_stop_check:\n",
    "        early_stop_temp=val_loss_list[-patience:]\n",
    "        if all(early_stop_temp[i]<early_stop_temp[i+1] for i in range (len(early_stop_temp)-1)):\n",
    "            print(\"Early stop!\")\n",
    "            break\n",
    "            \n",
    "    if epoch+1>saving_start_epoch:\n",
    "        if val_loss_list[-1]<val_loss_min:\n",
    "            if os.path.isfile(model_name):\n",
    "                os.remove(model_name)\n",
    "            val_loss_min=val_loss_list[-1]\n",
    "            model_name=\"Custom_model_\"+model_char+\"_{:.3f}\".format(val_loss_min)\n",
    "            torch.save(model, model_name)\n",
    "            print(\"Model replaced and saved as \",model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plt.ylabel(\"val_accuracy\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(val_acc_list)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_acc_list=np.array(val_acc_list)\n",
    "# np.savetxt(\"ver_2.0.txt\", val_acc_list, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import pandas as pd\n",
    "# import argparse\n",
    "# import time\n",
    "# import torchvision\n",
    "# import torchvision.transforms as transforms\n",
    "# from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize = transforms.Normalize(mean=[0.507, 0.487, 0.441],\n",
    "#                                      std=[0.267, 0.256, 0.276])\n",
    "# test_transform = transforms.Compose([\n",
    "#         transforms.ToTensor(),\n",
    "#         normalize\n",
    "#     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset = torchvision.datasets.ImageFolder('./data/test', transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category = []\n",
    "# for input, _ in test_loader:\n",
    "#     input = input.cuda()\n",
    "#     output = model(input)\n",
    "#     output = torch.argmax(output, dim=1)\n",
    "#     Category = Category + output.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Id = list(range(0, 8000))\n",
    "# samples = {\n",
    "#    'Id': Id,\n",
    "#    'Category': Category \n",
    "# }\n",
    "# df = pd.DataFrame(samples, columns=['Id', 'Category'])\n",
    "\n",
    "# df.to_csv('submission_2.0_2.csv', index=False)\n",
    "# print('Done!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
