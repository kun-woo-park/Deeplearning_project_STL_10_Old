{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import torch.distributed as dist\n",
    "import math\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import Type, Any, Callable, Union, List, Optional\n",
    "from torch import Tensor\n",
    "from torch.hub import load_state_dict_from_url\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.data.sampler import BatchSampler\n",
    "from collections.abc import Mapping, Sequence\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path_path= f\"{os.getcwd()}/data/split_train/train\"\n",
    "test_path_path= f\"{os.getcwd()}/data/split_train/val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=256\n",
    "num_gpus=4\n",
    "num_workers=8\n",
    "lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.507, 0.487, 0.441],\n",
    "                                     std=[0.267, 0.256, 0.276])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder(\n",
    "        train_path_path,\n",
    "        transforms.Compose([\n",
    "        transforms.RandomRotation(degrees=45),\n",
    "            \n",
    "        transforms.RandomResizedCrop(70),\n",
    "        transforms.ColorJitter(.3,.3,.3,.3),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "for _ in range (19):\n",
    "    aug_data = datasets.ImageFolder(\n",
    "        train_path_path,\n",
    "        transforms.Compose([\n",
    "        transforms.RandomRotation(degrees=45),\n",
    "            \n",
    "        transforms.RandomResizedCrop(70),\n",
    "        transforms.ColorJitter(.3,.3,.3,.3),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "    dataset=torch.utils.data.ConcatDataset([dataset,aug_data])\n",
    "valset = datasets.ImageFolder(\n",
    "        test_path_path,\n",
    "        transforms.Compose([\n",
    "        transforms.RandomRotation(degrees=45),\n",
    "            \n",
    "        transforms.RandomResizedCrop(70),\n",
    "        transforms.ColorJitter(.3,.3,.3,.3),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "for _ in range (19):\n",
    "    aug_data = datasets.ImageFolder(\n",
    "        test_path_path,\n",
    "        transforms.Compose([\n",
    "        transforms.RandomRotation(degrees=45),\n",
    "            \n",
    "        transforms.RandomResizedCrop(70),\n",
    "        transforms.ColorJitter(.3,.3,.3,.3),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "    valset=torch.utils.data.ConcatDataset([valset,aug_data])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=True,\n",
    "        num_workers=8, pin_memory=True,drop_last=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        valset, batch_size=batch_size, shuffle=True,\n",
    "        num_workers=8, pin_memory=True,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
    "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
    "    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n",
    "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
    "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
    "\n",
    "    expansion: int = 4\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: Optional[nn.Module] = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
    "    ) -> None:\n",
    "        super(Bottleneck, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        width = int(planes * (base_width / 64.)) * groups\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        block: Type[Union[Bottleneck]],\n",
    "        layers: List[int],\n",
    "        num_classes: int = 10,\n",
    "        zero_init_residual: bool = False,\n",
    "        groups: int = 1,\n",
    "        width_per_group: int = 64,\n",
    "        replace_stride_with_dilation: Optional[List[bool]] = None,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
    "    ) -> None:\n",
    "        super(ResNet, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
    "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[1])\n",
    "        \n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(256 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]\n",
    "\n",
    "    def _make_layer(self, block: Type[Union[Bottleneck]], planes: int, blocks: int,\n",
    "                    stride: int = 1, dilate: bool = False) -> nn.Sequential:\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "                            self.base_width, previous_dilation, norm_layer))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation,\n",
    "                                norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x,1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self._forward_impl(x)\n",
    "\n",
    "\n",
    "def _resnet(\n",
    "    arch: str,\n",
    "    block: Type[Union[Bottleneck]],\n",
    "    layers: List[int],\n",
    "    pretrained: bool,\n",
    "    progress: bool,\n",
    "    **kwargs: Any\n",
    ") -> ResNet:\n",
    "    model = ResNet(block, layers, **kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
    "                                              progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "def _resnext(arch, block, layers, pretrained, progress, **kwargs):\n",
    "    model = ResNet(block, layers, **kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = load_state_dict_from_url(model_urls[arch], progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model(pretrained: bool = False, progress: bool = True, **kwargs):\n",
    "    \n",
    "    kwargs['groups'] = 8\n",
    "    kwargs['width_per_group'] = 4\n",
    "    return _resnext('resnext', Bottleneck, [3, 6, 3], pretrained, progress, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print (name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 48, 48]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 48, 48]             128\n",
      "              ReLU-3           [-1, 64, 48, 48]               0\n",
      "         MaxPool2d-4           [-1, 64, 24, 24]               0\n",
      "            Conv2d-5           [-1, 32, 24, 24]           2,048\n",
      "       BatchNorm2d-6           [-1, 32, 24, 24]              64\n",
      "              ReLU-7           [-1, 32, 24, 24]               0\n",
      "            Conv2d-8           [-1, 32, 24, 24]           1,152\n",
      "       BatchNorm2d-9           [-1, 32, 24, 24]              64\n",
      "             ReLU-10           [-1, 32, 24, 24]               0\n",
      "           Conv2d-11          [-1, 256, 24, 24]           8,192\n",
      "      BatchNorm2d-12          [-1, 256, 24, 24]             512\n",
      "           Conv2d-13          [-1, 256, 24, 24]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 24, 24]             512\n",
      "             ReLU-15          [-1, 256, 24, 24]               0\n",
      "       Bottleneck-16          [-1, 256, 24, 24]               0\n",
      "           Conv2d-17           [-1, 32, 24, 24]           8,192\n",
      "      BatchNorm2d-18           [-1, 32, 24, 24]              64\n",
      "             ReLU-19           [-1, 32, 24, 24]               0\n",
      "           Conv2d-20           [-1, 32, 24, 24]           1,152\n",
      "      BatchNorm2d-21           [-1, 32, 24, 24]              64\n",
      "             ReLU-22           [-1, 32, 24, 24]               0\n",
      "           Conv2d-23          [-1, 256, 24, 24]           8,192\n",
      "      BatchNorm2d-24          [-1, 256, 24, 24]             512\n",
      "             ReLU-25          [-1, 256, 24, 24]               0\n",
      "       Bottleneck-26          [-1, 256, 24, 24]               0\n",
      "           Conv2d-27           [-1, 32, 24, 24]           8,192\n",
      "      BatchNorm2d-28           [-1, 32, 24, 24]              64\n",
      "             ReLU-29           [-1, 32, 24, 24]               0\n",
      "           Conv2d-30           [-1, 32, 24, 24]           1,152\n",
      "      BatchNorm2d-31           [-1, 32, 24, 24]              64\n",
      "             ReLU-32           [-1, 32, 24, 24]               0\n",
      "           Conv2d-33          [-1, 256, 24, 24]           8,192\n",
      "      BatchNorm2d-34          [-1, 256, 24, 24]             512\n",
      "             ReLU-35          [-1, 256, 24, 24]               0\n",
      "       Bottleneck-36          [-1, 256, 24, 24]               0\n",
      "           Conv2d-37           [-1, 64, 24, 24]          16,384\n",
      "      BatchNorm2d-38           [-1, 64, 24, 24]             128\n",
      "             ReLU-39           [-1, 64, 24, 24]               0\n",
      "           Conv2d-40           [-1, 64, 12, 12]           4,608\n",
      "      BatchNorm2d-41           [-1, 64, 12, 12]             128\n",
      "             ReLU-42           [-1, 64, 12, 12]               0\n",
      "           Conv2d-43          [-1, 512, 12, 12]          32,768\n",
      "      BatchNorm2d-44          [-1, 512, 12, 12]           1,024\n",
      "           Conv2d-45          [-1, 512, 12, 12]         131,072\n",
      "      BatchNorm2d-46          [-1, 512, 12, 12]           1,024\n",
      "             ReLU-47          [-1, 512, 12, 12]               0\n",
      "       Bottleneck-48          [-1, 512, 12, 12]               0\n",
      "           Conv2d-49           [-1, 64, 12, 12]          32,768\n",
      "      BatchNorm2d-50           [-1, 64, 12, 12]             128\n",
      "             ReLU-51           [-1, 64, 12, 12]               0\n",
      "           Conv2d-52           [-1, 64, 12, 12]           4,608\n",
      "      BatchNorm2d-53           [-1, 64, 12, 12]             128\n",
      "             ReLU-54           [-1, 64, 12, 12]               0\n",
      "           Conv2d-55          [-1, 512, 12, 12]          32,768\n",
      "      BatchNorm2d-56          [-1, 512, 12, 12]           1,024\n",
      "             ReLU-57          [-1, 512, 12, 12]               0\n",
      "       Bottleneck-58          [-1, 512, 12, 12]               0\n",
      "           Conv2d-59           [-1, 64, 12, 12]          32,768\n",
      "      BatchNorm2d-60           [-1, 64, 12, 12]             128\n",
      "             ReLU-61           [-1, 64, 12, 12]               0\n",
      "           Conv2d-62           [-1, 64, 12, 12]           4,608\n",
      "      BatchNorm2d-63           [-1, 64, 12, 12]             128\n",
      "             ReLU-64           [-1, 64, 12, 12]               0\n",
      "           Conv2d-65          [-1, 512, 12, 12]          32,768\n",
      "      BatchNorm2d-66          [-1, 512, 12, 12]           1,024\n",
      "             ReLU-67          [-1, 512, 12, 12]               0\n",
      "       Bottleneck-68          [-1, 512, 12, 12]               0\n",
      "           Conv2d-69           [-1, 64, 12, 12]          32,768\n",
      "      BatchNorm2d-70           [-1, 64, 12, 12]             128\n",
      "             ReLU-71           [-1, 64, 12, 12]               0\n",
      "           Conv2d-72           [-1, 64, 12, 12]           4,608\n",
      "      BatchNorm2d-73           [-1, 64, 12, 12]             128\n",
      "             ReLU-74           [-1, 64, 12, 12]               0\n",
      "           Conv2d-75          [-1, 512, 12, 12]          32,768\n",
      "      BatchNorm2d-76          [-1, 512, 12, 12]           1,024\n",
      "             ReLU-77          [-1, 512, 12, 12]               0\n",
      "       Bottleneck-78          [-1, 512, 12, 12]               0\n",
      "           Conv2d-79           [-1, 64, 12, 12]          32,768\n",
      "      BatchNorm2d-80           [-1, 64, 12, 12]             128\n",
      "             ReLU-81           [-1, 64, 12, 12]               0\n",
      "           Conv2d-82           [-1, 64, 12, 12]           4,608\n",
      "      BatchNorm2d-83           [-1, 64, 12, 12]             128\n",
      "             ReLU-84           [-1, 64, 12, 12]               0\n",
      "           Conv2d-85          [-1, 512, 12, 12]          32,768\n",
      "      BatchNorm2d-86          [-1, 512, 12, 12]           1,024\n",
      "             ReLU-87          [-1, 512, 12, 12]               0\n",
      "       Bottleneck-88          [-1, 512, 12, 12]               0\n",
      "           Conv2d-89           [-1, 64, 12, 12]          32,768\n",
      "      BatchNorm2d-90           [-1, 64, 12, 12]             128\n",
      "             ReLU-91           [-1, 64, 12, 12]               0\n",
      "           Conv2d-92           [-1, 64, 12, 12]           4,608\n",
      "      BatchNorm2d-93           [-1, 64, 12, 12]             128\n",
      "             ReLU-94           [-1, 64, 12, 12]               0\n",
      "           Conv2d-95          [-1, 512, 12, 12]          32,768\n",
      "      BatchNorm2d-96          [-1, 512, 12, 12]           1,024\n",
      "             ReLU-97          [-1, 512, 12, 12]               0\n",
      "       Bottleneck-98          [-1, 512, 12, 12]               0\n",
      "           Conv2d-99          [-1, 128, 12, 12]          65,536\n",
      "     BatchNorm2d-100          [-1, 128, 12, 12]             256\n",
      "            ReLU-101          [-1, 128, 12, 12]               0\n",
      "          Conv2d-102            [-1, 128, 6, 6]          18,432\n",
      "     BatchNorm2d-103            [-1, 128, 6, 6]             256\n",
      "            ReLU-104            [-1, 128, 6, 6]               0\n",
      "          Conv2d-105           [-1, 1024, 6, 6]         131,072\n",
      "     BatchNorm2d-106           [-1, 1024, 6, 6]           2,048\n",
      "          Conv2d-107           [-1, 1024, 6, 6]         524,288\n",
      "     BatchNorm2d-108           [-1, 1024, 6, 6]           2,048\n",
      "            ReLU-109           [-1, 1024, 6, 6]               0\n",
      "      Bottleneck-110           [-1, 1024, 6, 6]               0\n",
      "          Conv2d-111            [-1, 128, 6, 6]         131,072\n",
      "     BatchNorm2d-112            [-1, 128, 6, 6]             256\n",
      "            ReLU-113            [-1, 128, 6, 6]               0\n",
      "          Conv2d-114            [-1, 128, 6, 6]          18,432\n",
      "     BatchNorm2d-115            [-1, 128, 6, 6]             256\n",
      "            ReLU-116            [-1, 128, 6, 6]               0\n",
      "          Conv2d-117           [-1, 1024, 6, 6]         131,072\n",
      "     BatchNorm2d-118           [-1, 1024, 6, 6]           2,048\n",
      "            ReLU-119           [-1, 1024, 6, 6]               0\n",
      "      Bottleneck-120           [-1, 1024, 6, 6]               0\n",
      "          Conv2d-121            [-1, 128, 6, 6]         131,072\n",
      "     BatchNorm2d-122            [-1, 128, 6, 6]             256\n",
      "            ReLU-123            [-1, 128, 6, 6]               0\n",
      "          Conv2d-124            [-1, 128, 6, 6]          18,432\n",
      "     BatchNorm2d-125            [-1, 128, 6, 6]             256\n",
      "            ReLU-126            [-1, 128, 6, 6]               0\n",
      "          Conv2d-127           [-1, 1024, 6, 6]         131,072\n",
      "     BatchNorm2d-128           [-1, 1024, 6, 6]           2,048\n",
      "            ReLU-129           [-1, 1024, 6, 6]               0\n",
      "      Bottleneck-130           [-1, 1024, 6, 6]               0\n",
      "AdaptiveAvgPool2d-131           [-1, 1024, 1, 1]               0\n",
      "          Linear-132                   [-1, 10]          10,250\n",
      "================================================================\n",
      "Total params: 1,939,530\n",
      "Trainable params: 1,939,530\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.11\n",
      "Forward/backward pass size (MB): 44.62\n",
      "Params size (MB): 7.40\n",
      "Estimated Total Size (MB): 52.13\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model,(3,96,96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/11/15 02:44:35\n",
      "epoch: 1/200 | trn loss: 1.8938 | val loss: 1.7185 | val accuracy: 35.5769% \n",
      "\n",
      "2020/11/15 02:45:16\n",
      "epoch: 2/200 | trn loss: 1.6080 | val loss: 1.5593 | val accuracy: 42.8335% \n",
      "\n",
      "2020/11/15 02:45:56\n",
      "epoch: 3/200 | trn loss: 1.4550 | val loss: 1.4379 | val accuracy: 48.4726% \n",
      "\n",
      "2020/11/15 02:46:38\n",
      "epoch: 4/200 | trn loss: 1.3521 | val loss: 1.3812 | val accuracy: 50.3556% \n",
      "\n",
      "2020/11/15 02:47:18\n",
      "epoch: 5/200 | trn loss: 1.2467 | val loss: 1.3467 | val accuracy: 52.0783% \n",
      "\n",
      "2020/11/15 02:47:59\n",
      "epoch: 6/200 | trn loss: 1.1642 | val loss: 1.3077 | val accuracy: 53.7660% \n",
      "\n",
      "2020/11/15 02:48:41\n",
      "epoch: 7/200 | trn loss: 1.1043 | val loss: 1.2660 | val accuracy: 55.4287% \n",
      "\n",
      "2020/11/15 02:49:21\n",
      "epoch: 8/200 | trn loss: 1.0260 | val loss: 1.2176 | val accuracy: 57.3668% \n",
      "\n",
      "2020/11/15 02:49:59\n",
      "epoch: 9/200 | trn loss: 0.9670 | val loss: 1.2129 | val accuracy: 58.5036% \n",
      "\n",
      "2020/11/15 02:50:40\n",
      "epoch: 10/200 | trn loss: 0.9098 | val loss: 1.2058 | val accuracy: 58.5437% \n",
      "\n",
      "2020/11/15 02:51:22\n",
      "epoch: 11/200 | trn loss: 0.8699 | val loss: 1.2270 | val accuracy: 57.9327% \n",
      "\n",
      "Model replaced and saved as  /home/jupyter-deeplearning/res_modelCustom_model_2.0_1.227\n",
      "2020/11/15 02:52:05\n",
      "epoch: 12/200 | trn loss: 0.8191 | val loss: 1.2048 | val accuracy: 59.1647% \n",
      "\n",
      "Model replaced and saved as  /home/jupyter-deeplearning/res_modelCustom_model_2.0_1.205\n",
      "2020/11/15 02:52:44\n",
      "epoch: 13/200 | trn loss: 0.7788 | val loss: 1.2189 | val accuracy: 59.2248% \n",
      "\n",
      "2020/11/15 02:53:22\n",
      "epoch: 14/200 | trn loss: 0.7586 | val loss: 1.2017 | val accuracy: 60.0210% \n",
      "\n",
      "Model replaced and saved as  /home/jupyter-deeplearning/res_modelCustom_model_2.0_1.202\n",
      "2020/11/15 02:54:02\n",
      "epoch: 15/200 | trn loss: 0.7069 | val loss: 1.2335 | val accuracy: 60.0761% \n",
      "\n",
      "2020/11/15 02:54:42\n",
      "epoch: 16/200 | trn loss: 0.6884 | val loss: 1.1918 | val accuracy: 60.4918% \n",
      "\n",
      "Model replaced and saved as  /home/jupyter-deeplearning/res_modelCustom_model_2.0_1.192\n",
      "2020/11/15 02:55:19\n",
      "epoch: 17/200 | trn loss: 0.6509 | val loss: 1.1783 | val accuracy: 61.6987% \n",
      "\n",
      "Model replaced and saved as  /home/jupyter-deeplearning/res_modelCustom_model_2.0_1.178\n",
      "2020/11/15 02:55:58\n",
      "epoch: 18/200 | trn loss: 0.6308 | val loss: 1.2214 | val accuracy: 61.3782% \n",
      "\n",
      "2020/11/15 02:56:38\n",
      "epoch: 19/200 | trn loss: 0.6003 | val loss: 1.2417 | val accuracy: 61.1829% \n",
      "\n",
      "2020/11/15 02:57:17\n",
      "epoch: 20/200 | trn loss: 0.5851 | val loss: 1.1950 | val accuracy: 62.1094% \n",
      "\n",
      "2020/11/15 02:58:01\n",
      "epoch: 21/200 | trn loss: 0.5642 | val loss: 1.2833 | val accuracy: 60.7522% \n",
      "\n",
      "2020/11/15 02:58:43\n",
      "epoch: 22/200 | trn loss: 0.5393 | val loss: 1.1914 | val accuracy: 62.5751% \n",
      "\n",
      "2020/11/15 02:59:25\n",
      "epoch: 23/200 | trn loss: 0.5271 | val loss: 1.2532 | val accuracy: 61.5284% \n",
      "\n",
      "2020/11/15 03:00:05\n",
      "epoch: 24/200 | trn loss: 0.5064 | val loss: 1.2645 | val accuracy: 61.3782% \n",
      "\n",
      "2020/11/15 03:00:46\n",
      "epoch: 25/200 | trn loss: 0.4966 | val loss: 1.2294 | val accuracy: 62.8405% \n",
      "\n",
      "2020/11/15 03:01:27\n",
      "epoch: 26/200 | trn loss: 0.4776 | val loss: 1.2828 | val accuracy: 61.7037% \n",
      "\n",
      "2020/11/15 03:02:09\n",
      "epoch: 27/200 | trn loss: 0.4681 | val loss: 1.2453 | val accuracy: 62.7053% \n",
      "\n",
      "2020/11/15 03:02:47\n",
      "epoch: 28/200 | trn loss: 0.4520 | val loss: 1.2908 | val accuracy: 61.0226% \n",
      "\n",
      "2020/11/15 03:03:27\n",
      "epoch: 29/200 | trn loss: 0.4427 | val loss: 1.2505 | val accuracy: 62.8105% \n",
      "\n",
      "2020/11/15 03:04:08\n",
      "epoch: 30/200 | trn loss: 0.4314 | val loss: 1.2652 | val accuracy: 62.8806% \n",
      "\n",
      "2020/11/15 03:04:47\n",
      "epoch: 31/200 | trn loss: 0.3411 | val loss: 1.1888 | val accuracy: 65.2945% \n",
      "\n",
      "2020/11/15 03:05:23\n",
      "epoch: 32/200 | trn loss: 0.3131 | val loss: 1.1900 | val accuracy: 65.1943% \n",
      "\n",
      "2020/11/15 03:05:59\n",
      "epoch: 33/200 | trn loss: 0.3024 | val loss: 1.2112 | val accuracy: 65.4397% \n",
      "\n",
      "2020/11/15 03:06:35\n",
      "epoch: 34/200 | trn loss: 0.2876 | val loss: 1.1994 | val accuracy: 65.4597% \n",
      "\n",
      "2020/11/15 03:07:10\n",
      "epoch: 35/200 | trn loss: 0.2809 | val loss: 1.2172 | val accuracy: 65.2093% \n",
      "\n",
      "2020/11/15 03:07:45\n",
      "epoch: 36/200 | trn loss: 0.2804 | val loss: 1.2190 | val accuracy: 65.7101% \n",
      "\n",
      "2020/11/15 03:08:21\n",
      "epoch: 37/200 | trn loss: 0.2750 | val loss: 1.2203 | val accuracy: 65.6500% \n",
      "\n",
      "2020/11/15 03:08:56\n",
      "epoch: 38/200 | trn loss: 0.2700 | val loss: 1.2345 | val accuracy: 65.2845% \n",
      "\n",
      "2020/11/15 03:09:31\n",
      "epoch: 39/200 | trn loss: 0.2688 | val loss: 1.2493 | val accuracy: 65.8904% \n",
      "\n",
      "2020/11/15 03:10:07\n",
      "epoch: 40/200 | trn loss: 0.2623 | val loss: 1.2800 | val accuracy: 64.8488% \n",
      "\n",
      "2020/11/15 03:10:42\n",
      "epoch: 41/200 | trn loss: 0.2558 | val loss: 1.2468 | val accuracy: 65.8353% \n",
      "\n",
      "2020/11/15 03:11:18\n",
      "epoch: 42/200 | trn loss: 0.2507 | val loss: 1.2834 | val accuracy: 65.3145% \n",
      "\n"
     ]
    }
   ],
   "source": [
    "saving_path=\"/home/jupyter-deeplearning/res_model\"\n",
    "trn_loss_list = []\n",
    "val_loss_list = []\n",
    "val_acc_list = []\n",
    "total_epoch=200\n",
    "model_char=\"2.0\"\n",
    "model_name=\"\"\n",
    "patience=10\n",
    "start_early_stop_check=0\n",
    "saving_start_epoch=10\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    trn_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        if torch.cuda.is_available():\n",
    "            inputs=inputs.cuda()\n",
    "            labels=labels.cuda()\n",
    "        # grad init\n",
    "        optimizer.zero_grad()\n",
    "        # forward propagation\n",
    "        output= model(inputs)\n",
    "        # calculate loss\n",
    "        loss=criterion(output, labels)\n",
    "        # back propagation \n",
    "        loss.backward()\n",
    "        # weight update\n",
    "        optimizer.step()\n",
    "        \n",
    "        # trn_loss summary\n",
    "        trn_loss += loss.item()\n",
    "        # del (memory issue)\n",
    "        del loss\n",
    "        del output\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        cor_match = 0\n",
    "        for j, val in enumerate(test_loader):\n",
    "            val_x, val_label = val\n",
    "            if torch.cuda.is_available():\n",
    "                val_x = val_x.cuda()\n",
    "                val_label =val_label.cuda()\n",
    "            val_output = model(val_x)\n",
    "            v_loss = criterion(val_output, val_label)\n",
    "            val_loss += v_loss\n",
    "            _, predicted=torch.max(val_output,1)\n",
    "            cor_match+=np.count_nonzero(predicted.cpu().detach()==val_label.cpu().detach())\n",
    "    del val_output\n",
    "    del v_loss\n",
    "    del predicted\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    \n",
    "    \n",
    "    trn_loss_list.append(trn_loss/len(train_loader))\n",
    "    val_loss_list.append(val_loss/len(test_loader))\n",
    "    val_acc=cor_match/(len(test_loader)*batch_size)\n",
    "    val_acc_list.append(val_acc)\n",
    "    now = time.localtime()\n",
    "    print (\"%04d/%02d/%02d %02d:%02d:%02d\" % (now.tm_year, now.tm_mon, now.tm_mday, now.tm_hour, now.tm_min, now.tm_sec))\n",
    "\n",
    "    print(\"epoch: {}/{} | trn loss: {:.4f} | val loss: {:.4f} | val accuracy: {:.4f}% \\n\".format(\n",
    "                epoch+1, total_epoch, trn_loss / len(train_loader), val_loss / len(test_loader), val_acc*100\n",
    "            ))\n",
    "    \n",
    "    \n",
    "    if epoch+1>2:\n",
    "        if val_loss_list[-1]>val_loss_list[-2]:\n",
    "            start_early_stop_check=1\n",
    "    else:\n",
    "        val_loss_min=val_loss_list[-1]\n",
    "        \n",
    "    if start_early_stop_check:\n",
    "        early_stop_temp=val_loss_list[-patience:]\n",
    "        if all(early_stop_temp[i]<early_stop_temp[i+1] for i in range (len(early_stop_temp)-1)):\n",
    "            print(\"Early stop!\")\n",
    "            break\n",
    "            \n",
    "    if epoch+1>saving_start_epoch:\n",
    "        if val_loss_list[-1]<val_loss_min:\n",
    "            if os.path.isfile(model_name):\n",
    "                os.remove(model_name)\n",
    "            val_loss_min=val_loss_list[-1]\n",
    "            model_name=saving_path+\"Custom_model_\"+model_char+\"_{:.3f}\".format(val_loss_min)\n",
    "            torch.save(model, model_name)\n",
    "            print(\"Model replaced and saved as \",model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder(\n",
    "        train_path_path,\n",
    "        transforms.Compose([\n",
    "        transforms.RandomRotation(degrees=45),\n",
    "        transforms.Resize(120),    \n",
    "        transforms.ColorJitter(.3,.3,.3,.3),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "for _ in range (19):\n",
    "    aug_data = datasets.ImageFolder(\n",
    "        train_path_path,\n",
    "        transforms.Compose([\n",
    "        transforms.RandomRotation(degrees=45),\n",
    "        transforms.Resize(120),    \n",
    "        transforms.ColorJitter(.3,.3,.3,.3),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "    dataset=torch.utils.data.ConcatDataset([dataset,aug_data])\n",
    "valset = datasets.ImageFolder(\n",
    "        test_path_path,\n",
    "        transforms.Compose([\n",
    "        transforms.RandomRotation(degrees=45),\n",
    "        transforms.Resize(120),\n",
    "        transforms.ColorJitter(.3,.3,.3,.3),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "for _ in range (19):\n",
    "    aug_data = datasets.ImageFolder(\n",
    "        test_path_path,\n",
    "        transforms.Compose([\n",
    "        transforms.RandomRotation(degrees=45),\n",
    "        transforms.Resize(120),    \n",
    "        transforms.ColorJitter(.3,.3,.3,.3),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "    valset=torch.utils.data.ConcatDataset([valset,aug_data])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=True,\n",
    "        num_workers=8, pin_memory=True,drop_last=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        valset, batch_size=batch_size, shuffle=True,\n",
    "        num_workers=8, pin_memory=True,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fine tuning\n",
    "\n",
    "total_epoch=100\n",
    "model_char=\"2.0\"\n",
    "model_name=\"\"\n",
    "patience=10\n",
    "start_early_stop_check=0\n",
    "saving_start_epoch=10\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    trn_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        if torch.cuda.is_available():\n",
    "            inputs=inputs.cuda()\n",
    "            labels=labels.cuda()\n",
    "        # grad init\n",
    "        optimizer.zero_grad()\n",
    "        # forward propagation\n",
    "        output= model(inputs)\n",
    "        # calculate loss\n",
    "        loss=criterion(output, labels)\n",
    "        # back propagation \n",
    "        loss.backward()\n",
    "        # weight update\n",
    "        optimizer.step()\n",
    "        \n",
    "        # trn_loss summary\n",
    "        trn_loss += loss.item()\n",
    "        # del (memory issue)\n",
    "        del loss\n",
    "        del output\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        cor_match = 0\n",
    "        for j, val in enumerate(test_loader):\n",
    "            val_x, val_label = val\n",
    "            if torch.cuda.is_available():\n",
    "                val_x = val_x.cuda()\n",
    "                val_label =val_label.cuda()\n",
    "            val_output = model(val_x)\n",
    "            v_loss = criterion(val_output, val_label)\n",
    "            val_loss += v_loss\n",
    "            _, predicted=torch.max(val_output,1)\n",
    "            cor_match+=np.count_nonzero(predicted.cpu().detach()==val_label.cpu().detach())\n",
    "    del val_output\n",
    "    del v_loss\n",
    "    del predicted\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    \n",
    "    \n",
    "    trn_loss_list.append(trn_loss/len(train_loader))\n",
    "    val_loss_list.append(val_loss/len(test_loader))\n",
    "    val_acc=cor_match/(len(test_loader)*batch_size)\n",
    "    val_acc_list.append(val_acc)\n",
    "    now = time.localtime()\n",
    "    print (\"%04d/%02d/%02d %02d:%02d:%02d\" % (now.tm_year, now.tm_mon, now.tm_mday, now.tm_hour, now.tm_min, now.tm_sec))\n",
    "\n",
    "    print(\"epoch: {}/{} | trn loss: {:.4f} | val loss: {:.4f} | val accuracy: {:.4f}% \\n\".format(\n",
    "                epoch+1, total_epoch, trn_loss / len(train_loader), val_loss / len(test_loader), val_acc*100\n",
    "            ))\n",
    "    \n",
    "    \n",
    "    if epoch+1>2:\n",
    "        if val_loss_list[-1]>val_loss_list[-2]:\n",
    "            start_early_stop_check=1\n",
    "    else:\n",
    "        val_loss_min=val_loss_list[-1]\n",
    "        \n",
    "    if start_early_stop_check:\n",
    "        early_stop_temp=val_loss_list[-patience:]\n",
    "        if all(early_stop_temp[i]<early_stop_temp[i+1] for i in range (len(early_stop_temp)-1)):\n",
    "            print(\"Early stop!\")\n",
    "            break\n",
    "            \n",
    "    if epoch+1>saving_start_epoch:\n",
    "        if val_loss_list[-1]<val_loss_min:\n",
    "            if os.path.isfile(model_name):\n",
    "                os.remove(model_name)\n",
    "            val_loss_min=val_loss_list[-1]\n",
    "            model_name=\"Custom_model_\"+model_char+\"_{:.3f}\".format(val_loss_min)\n",
    "            torch.save(model, model_name)\n",
    "            print(\"Model replaced and saved as \",model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plt.ylabel(\"val_accuracy\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(val_acc_list)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_acc_list=np.array(val_acc_list)\n",
    "np.savetxt(\"ver_2.0.txt\", val_acc_list, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import time\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.507, 0.487, 0.441],\n",
    "                                     std=[0.267, 0.256, 0.276])\n",
    "test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = torchvision.datasets.ImageFolder('./data/test', transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Category = []\n",
    "for input, _ in test_loader:\n",
    "    input = input.cuda()\n",
    "    output = model(input)\n",
    "    output = torch.argmax(output, dim=1)\n",
    "    Category = Category + output.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Id = list(range(0, 8000))\n",
    "samples = {\n",
    "   'Id': Id,\n",
    "   'Category': Category \n",
    "}\n",
    "df = pd.DataFrame(samples, columns=['Id', 'Category'])\n",
    "\n",
    "df.to_csv('submission_2.0_2.csv', index=False)\n",
    "print('Done!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
