{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import torch.distributed as dist\n",
    "import math\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import Type, Any, Callable, Union, List, Optional\n",
    "from torch import Tensor\n",
    "from torch.hub import load_state_dict_from_url\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.data.sampler import BatchSampler\n",
    "from collections.abc import Mapping, Sequence\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path_path= f\"../data/split_train/train\"\n",
    "test_path_path= f\"../data/split_train/val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "num_gpus=4\n",
    "num_workers=8\n",
    "lr=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.507, 0.487, 0.441],\n",
    "                                     std=[0.267, 0.256, 0.276])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder(\n",
    "        train_path_path,\n",
    "        transforms.Compose([\n",
    "        transforms.RandomRotation(degrees=45),\n",
    "            \n",
    "        transforms.RandomResizedCrop(70),\n",
    "        transforms.ColorJitter(.3,.3,.3,.3),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "for _ in range (19):\n",
    "    aug_data = datasets.ImageFolder(\n",
    "        train_path_path,\n",
    "        transforms.Compose([\n",
    "        transforms.RandomRotation(degrees=45),\n",
    "            \n",
    "        transforms.RandomResizedCrop(70),\n",
    "        transforms.ColorJitter(.3,.3,.3,.3),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "    dataset=torch.utils.data.ConcatDataset([dataset,aug_data])\n",
    "valset = datasets.ImageFolder(\n",
    "        test_path_path,\n",
    "        transforms.Compose([\n",
    "        transforms.RandomRotation(degrees=45),\n",
    "            \n",
    "        transforms.RandomResizedCrop(70),\n",
    "        transforms.ColorJitter(.3,.3,.3,.3),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "for _ in range (19):\n",
    "    aug_data = datasets.ImageFolder(\n",
    "        test_path_path,\n",
    "        transforms.Compose([\n",
    "        transforms.RandomRotation(degrees=45),\n",
    "            \n",
    "        transforms.RandomResizedCrop(70),\n",
    "        transforms.ColorJitter(.3,.3,.3,.3),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "    valset=torch.utils.data.ConcatDataset([valset,aug_data])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=True,\n",
    "        num_workers=8, pin_memory=True,drop_last=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        valset, batch_size=batch_size, shuffle=True,\n",
    "        num_workers=8, pin_memory=True,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
    "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
    "    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n",
    "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
    "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
    "\n",
    "    expansion: int = 4\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: Optional[nn.Module] = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
    "    ) -> None:\n",
    "        super(Bottleneck, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        width = int(planes * (base_width / 64.)) * groups\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.LeakyReLU(0.1)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        block: Type[Union[Bottleneck]],\n",
    "        layers: List[int],\n",
    "        num_classes: int = 10,\n",
    "        zero_init_residual: bool = False,\n",
    "        groups: int = 1,\n",
    "        width_per_group: int = 64,\n",
    "        replace_stride_with_dilation: Optional[List[bool]] = None,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
    "    ) -> None:\n",
    "        super(ResNet, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
    "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=3, stride=1, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.LeakyReLU(0.1)\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 30, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 58, layers[1], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 90, layers[2], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[1])\n",
    "        \n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(90 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]\n",
    "\n",
    "    def _make_layer(self, block: Type[Union[Bottleneck]], planes: int, blocks: int,\n",
    "                    stride: int = 1, dilate: bool = False) -> nn.Sequential:\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "                            self.base_width, previous_dilation, norm_layer))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation,\n",
    "                                norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x,1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self._forward_impl(x)\n",
    "\n",
    "\n",
    "def _resnet(\n",
    "    arch: str,\n",
    "    block: Type[Union[Bottleneck]],\n",
    "    layers: List[int],\n",
    "    pretrained: bool,\n",
    "    progress: bool,\n",
    "    **kwargs: Any\n",
    ") -> ResNet:\n",
    "    model = ResNet(block, layers, **kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
    "                                              progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "def _resnext(arch, block, layers, pretrained, progress, **kwargs):\n",
    "    model = ResNet(block, layers, **kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = load_state_dict_from_url(model_urls[arch], progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model(pretrained: bool = False, progress: bool = True, **kwargs):\n",
    "    \n",
    "    kwargs['groups'] = 1\n",
    "    kwargs['width_per_group'] = 64\n",
    "    return _resnext('resnext', Bottleneck, [8, 8, 8], pretrained, progress, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print (name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 100, 100]           1,728\n",
      "       BatchNorm2d-2         [-1, 64, 100, 100]             128\n",
      "         LeakyReLU-3         [-1, 64, 100, 100]               0\n",
      "         MaxPool2d-4           [-1, 64, 50, 50]               0\n",
      "            Conv2d-5           [-1, 30, 50, 50]           1,920\n",
      "       BatchNorm2d-6           [-1, 30, 50, 50]              60\n",
      "         LeakyReLU-7           [-1, 30, 50, 50]               0\n",
      "            Conv2d-8           [-1, 30, 50, 50]           8,100\n",
      "       BatchNorm2d-9           [-1, 30, 50, 50]              60\n",
      "        LeakyReLU-10           [-1, 30, 50, 50]               0\n",
      "           Conv2d-11          [-1, 120, 50, 50]           3,600\n",
      "      BatchNorm2d-12          [-1, 120, 50, 50]             240\n",
      "           Conv2d-13          [-1, 120, 50, 50]           7,680\n",
      "      BatchNorm2d-14          [-1, 120, 50, 50]             240\n",
      "        LeakyReLU-15          [-1, 120, 50, 50]               0\n",
      "       Bottleneck-16          [-1, 120, 50, 50]               0\n",
      "           Conv2d-17           [-1, 30, 50, 50]           3,600\n",
      "      BatchNorm2d-18           [-1, 30, 50, 50]              60\n",
      "        LeakyReLU-19           [-1, 30, 50, 50]               0\n",
      "           Conv2d-20           [-1, 30, 50, 50]           8,100\n",
      "      BatchNorm2d-21           [-1, 30, 50, 50]              60\n",
      "        LeakyReLU-22           [-1, 30, 50, 50]               0\n",
      "           Conv2d-23          [-1, 120, 50, 50]           3,600\n",
      "      BatchNorm2d-24          [-1, 120, 50, 50]             240\n",
      "        LeakyReLU-25          [-1, 120, 50, 50]               0\n",
      "       Bottleneck-26          [-1, 120, 50, 50]               0\n",
      "           Conv2d-27           [-1, 30, 50, 50]           3,600\n",
      "      BatchNorm2d-28           [-1, 30, 50, 50]              60\n",
      "        LeakyReLU-29           [-1, 30, 50, 50]               0\n",
      "           Conv2d-30           [-1, 30, 50, 50]           8,100\n",
      "      BatchNorm2d-31           [-1, 30, 50, 50]              60\n",
      "        LeakyReLU-32           [-1, 30, 50, 50]               0\n",
      "           Conv2d-33          [-1, 120, 50, 50]           3,600\n",
      "      BatchNorm2d-34          [-1, 120, 50, 50]             240\n",
      "        LeakyReLU-35          [-1, 120, 50, 50]               0\n",
      "       Bottleneck-36          [-1, 120, 50, 50]               0\n",
      "           Conv2d-37           [-1, 30, 50, 50]           3,600\n",
      "      BatchNorm2d-38           [-1, 30, 50, 50]              60\n",
      "        LeakyReLU-39           [-1, 30, 50, 50]               0\n",
      "           Conv2d-40           [-1, 30, 50, 50]           8,100\n",
      "      BatchNorm2d-41           [-1, 30, 50, 50]              60\n",
      "        LeakyReLU-42           [-1, 30, 50, 50]               0\n",
      "           Conv2d-43          [-1, 120, 50, 50]           3,600\n",
      "      BatchNorm2d-44          [-1, 120, 50, 50]             240\n",
      "        LeakyReLU-45          [-1, 120, 50, 50]               0\n",
      "       Bottleneck-46          [-1, 120, 50, 50]               0\n",
      "           Conv2d-47           [-1, 30, 50, 50]           3,600\n",
      "      BatchNorm2d-48           [-1, 30, 50, 50]              60\n",
      "        LeakyReLU-49           [-1, 30, 50, 50]               0\n",
      "           Conv2d-50           [-1, 30, 50, 50]           8,100\n",
      "      BatchNorm2d-51           [-1, 30, 50, 50]              60\n",
      "        LeakyReLU-52           [-1, 30, 50, 50]               0\n",
      "           Conv2d-53          [-1, 120, 50, 50]           3,600\n",
      "      BatchNorm2d-54          [-1, 120, 50, 50]             240\n",
      "        LeakyReLU-55          [-1, 120, 50, 50]               0\n",
      "       Bottleneck-56          [-1, 120, 50, 50]               0\n",
      "           Conv2d-57           [-1, 30, 50, 50]           3,600\n",
      "      BatchNorm2d-58           [-1, 30, 50, 50]              60\n",
      "        LeakyReLU-59           [-1, 30, 50, 50]               0\n",
      "           Conv2d-60           [-1, 30, 50, 50]           8,100\n",
      "      BatchNorm2d-61           [-1, 30, 50, 50]              60\n",
      "        LeakyReLU-62           [-1, 30, 50, 50]               0\n",
      "           Conv2d-63          [-1, 120, 50, 50]           3,600\n",
      "      BatchNorm2d-64          [-1, 120, 50, 50]             240\n",
      "        LeakyReLU-65          [-1, 120, 50, 50]               0\n",
      "       Bottleneck-66          [-1, 120, 50, 50]               0\n",
      "           Conv2d-67           [-1, 30, 50, 50]           3,600\n",
      "      BatchNorm2d-68           [-1, 30, 50, 50]              60\n",
      "        LeakyReLU-69           [-1, 30, 50, 50]               0\n",
      "           Conv2d-70           [-1, 30, 50, 50]           8,100\n",
      "      BatchNorm2d-71           [-1, 30, 50, 50]              60\n",
      "        LeakyReLU-72           [-1, 30, 50, 50]               0\n",
      "           Conv2d-73          [-1, 120, 50, 50]           3,600\n",
      "      BatchNorm2d-74          [-1, 120, 50, 50]             240\n",
      "        LeakyReLU-75          [-1, 120, 50, 50]               0\n",
      "       Bottleneck-76          [-1, 120, 50, 50]               0\n",
      "           Conv2d-77           [-1, 30, 50, 50]           3,600\n",
      "      BatchNorm2d-78           [-1, 30, 50, 50]              60\n",
      "        LeakyReLU-79           [-1, 30, 50, 50]               0\n",
      "           Conv2d-80           [-1, 30, 50, 50]           8,100\n",
      "      BatchNorm2d-81           [-1, 30, 50, 50]              60\n",
      "        LeakyReLU-82           [-1, 30, 50, 50]               0\n",
      "           Conv2d-83          [-1, 120, 50, 50]           3,600\n",
      "      BatchNorm2d-84          [-1, 120, 50, 50]             240\n",
      "        LeakyReLU-85          [-1, 120, 50, 50]               0\n",
      "       Bottleneck-86          [-1, 120, 50, 50]               0\n",
      "           Conv2d-87           [-1, 58, 50, 50]           6,960\n",
      "      BatchNorm2d-88           [-1, 58, 50, 50]             116\n",
      "        LeakyReLU-89           [-1, 58, 50, 50]               0\n",
      "           Conv2d-90           [-1, 58, 25, 25]          30,276\n",
      "      BatchNorm2d-91           [-1, 58, 25, 25]             116\n",
      "        LeakyReLU-92           [-1, 58, 25, 25]               0\n",
      "           Conv2d-93          [-1, 232, 25, 25]          13,456\n",
      "      BatchNorm2d-94          [-1, 232, 25, 25]             464\n",
      "           Conv2d-95          [-1, 232, 25, 25]          27,840\n",
      "      BatchNorm2d-96          [-1, 232, 25, 25]             464\n",
      "        LeakyReLU-97          [-1, 232, 25, 25]               0\n",
      "       Bottleneck-98          [-1, 232, 25, 25]               0\n",
      "           Conv2d-99           [-1, 58, 25, 25]          13,456\n",
      "     BatchNorm2d-100           [-1, 58, 25, 25]             116\n",
      "       LeakyReLU-101           [-1, 58, 25, 25]               0\n",
      "          Conv2d-102           [-1, 58, 25, 25]          30,276\n",
      "     BatchNorm2d-103           [-1, 58, 25, 25]             116\n",
      "       LeakyReLU-104           [-1, 58, 25, 25]               0\n",
      "          Conv2d-105          [-1, 232, 25, 25]          13,456\n",
      "     BatchNorm2d-106          [-1, 232, 25, 25]             464\n",
      "       LeakyReLU-107          [-1, 232, 25, 25]               0\n",
      "      Bottleneck-108          [-1, 232, 25, 25]               0\n",
      "          Conv2d-109           [-1, 58, 25, 25]          13,456\n",
      "     BatchNorm2d-110           [-1, 58, 25, 25]             116\n",
      "       LeakyReLU-111           [-1, 58, 25, 25]               0\n",
      "          Conv2d-112           [-1, 58, 25, 25]          30,276\n",
      "     BatchNorm2d-113           [-1, 58, 25, 25]             116\n",
      "       LeakyReLU-114           [-1, 58, 25, 25]               0\n",
      "          Conv2d-115          [-1, 232, 25, 25]          13,456\n",
      "     BatchNorm2d-116          [-1, 232, 25, 25]             464\n",
      "       LeakyReLU-117          [-1, 232, 25, 25]               0\n",
      "      Bottleneck-118          [-1, 232, 25, 25]               0\n",
      "          Conv2d-119           [-1, 58, 25, 25]          13,456\n",
      "     BatchNorm2d-120           [-1, 58, 25, 25]             116\n",
      "       LeakyReLU-121           [-1, 58, 25, 25]               0\n",
      "          Conv2d-122           [-1, 58, 25, 25]          30,276\n",
      "     BatchNorm2d-123           [-1, 58, 25, 25]             116\n",
      "       LeakyReLU-124           [-1, 58, 25, 25]               0\n",
      "          Conv2d-125          [-1, 232, 25, 25]          13,456\n",
      "     BatchNorm2d-126          [-1, 232, 25, 25]             464\n",
      "       LeakyReLU-127          [-1, 232, 25, 25]               0\n",
      "      Bottleneck-128          [-1, 232, 25, 25]               0\n",
      "          Conv2d-129           [-1, 58, 25, 25]          13,456\n",
      "     BatchNorm2d-130           [-1, 58, 25, 25]             116\n",
      "       LeakyReLU-131           [-1, 58, 25, 25]               0\n",
      "          Conv2d-132           [-1, 58, 25, 25]          30,276\n",
      "     BatchNorm2d-133           [-1, 58, 25, 25]             116\n",
      "       LeakyReLU-134           [-1, 58, 25, 25]               0\n",
      "          Conv2d-135          [-1, 232, 25, 25]          13,456\n",
      "     BatchNorm2d-136          [-1, 232, 25, 25]             464\n",
      "       LeakyReLU-137          [-1, 232, 25, 25]               0\n",
      "      Bottleneck-138          [-1, 232, 25, 25]               0\n",
      "          Conv2d-139           [-1, 58, 25, 25]          13,456\n",
      "     BatchNorm2d-140           [-1, 58, 25, 25]             116\n",
      "       LeakyReLU-141           [-1, 58, 25, 25]               0\n",
      "          Conv2d-142           [-1, 58, 25, 25]          30,276\n",
      "     BatchNorm2d-143           [-1, 58, 25, 25]             116\n",
      "       LeakyReLU-144           [-1, 58, 25, 25]               0\n",
      "          Conv2d-145          [-1, 232, 25, 25]          13,456\n",
      "     BatchNorm2d-146          [-1, 232, 25, 25]             464\n",
      "       LeakyReLU-147          [-1, 232, 25, 25]               0\n",
      "      Bottleneck-148          [-1, 232, 25, 25]               0\n",
      "          Conv2d-149           [-1, 58, 25, 25]          13,456\n",
      "     BatchNorm2d-150           [-1, 58, 25, 25]             116\n",
      "       LeakyReLU-151           [-1, 58, 25, 25]               0\n",
      "          Conv2d-152           [-1, 58, 25, 25]          30,276\n",
      "     BatchNorm2d-153           [-1, 58, 25, 25]             116\n",
      "       LeakyReLU-154           [-1, 58, 25, 25]               0\n",
      "          Conv2d-155          [-1, 232, 25, 25]          13,456\n",
      "     BatchNorm2d-156          [-1, 232, 25, 25]             464\n",
      "       LeakyReLU-157          [-1, 232, 25, 25]               0\n",
      "      Bottleneck-158          [-1, 232, 25, 25]               0\n",
      "          Conv2d-159           [-1, 58, 25, 25]          13,456\n",
      "     BatchNorm2d-160           [-1, 58, 25, 25]             116\n",
      "       LeakyReLU-161           [-1, 58, 25, 25]               0\n",
      "          Conv2d-162           [-1, 58, 25, 25]          30,276\n",
      "     BatchNorm2d-163           [-1, 58, 25, 25]             116\n",
      "       LeakyReLU-164           [-1, 58, 25, 25]               0\n",
      "          Conv2d-165          [-1, 232, 25, 25]          13,456\n",
      "     BatchNorm2d-166          [-1, 232, 25, 25]             464\n",
      "       LeakyReLU-167          [-1, 232, 25, 25]               0\n",
      "      Bottleneck-168          [-1, 232, 25, 25]               0\n",
      "          Conv2d-169           [-1, 90, 25, 25]          20,880\n",
      "     BatchNorm2d-170           [-1, 90, 25, 25]             180\n",
      "       LeakyReLU-171           [-1, 90, 25, 25]               0\n",
      "          Conv2d-172           [-1, 90, 13, 13]          72,900\n",
      "     BatchNorm2d-173           [-1, 90, 13, 13]             180\n",
      "       LeakyReLU-174           [-1, 90, 13, 13]               0\n",
      "          Conv2d-175          [-1, 360, 13, 13]          32,400\n",
      "     BatchNorm2d-176          [-1, 360, 13, 13]             720\n",
      "          Conv2d-177          [-1, 360, 13, 13]          83,520\n",
      "     BatchNorm2d-178          [-1, 360, 13, 13]             720\n",
      "       LeakyReLU-179          [-1, 360, 13, 13]               0\n",
      "      Bottleneck-180          [-1, 360, 13, 13]               0\n",
      "          Conv2d-181           [-1, 90, 13, 13]          32,400\n",
      "     BatchNorm2d-182           [-1, 90, 13, 13]             180\n",
      "       LeakyReLU-183           [-1, 90, 13, 13]               0\n",
      "          Conv2d-184           [-1, 90, 13, 13]          72,900\n",
      "     BatchNorm2d-185           [-1, 90, 13, 13]             180\n",
      "       LeakyReLU-186           [-1, 90, 13, 13]               0\n",
      "          Conv2d-187          [-1, 360, 13, 13]          32,400\n",
      "     BatchNorm2d-188          [-1, 360, 13, 13]             720\n",
      "       LeakyReLU-189          [-1, 360, 13, 13]               0\n",
      "      Bottleneck-190          [-1, 360, 13, 13]               0\n",
      "          Conv2d-191           [-1, 90, 13, 13]          32,400\n",
      "     BatchNorm2d-192           [-1, 90, 13, 13]             180\n",
      "       LeakyReLU-193           [-1, 90, 13, 13]               0\n",
      "          Conv2d-194           [-1, 90, 13, 13]          72,900\n",
      "     BatchNorm2d-195           [-1, 90, 13, 13]             180\n",
      "       LeakyReLU-196           [-1, 90, 13, 13]               0\n",
      "          Conv2d-197          [-1, 360, 13, 13]          32,400\n",
      "     BatchNorm2d-198          [-1, 360, 13, 13]             720\n",
      "       LeakyReLU-199          [-1, 360, 13, 13]               0\n",
      "      Bottleneck-200          [-1, 360, 13, 13]               0\n",
      "          Conv2d-201           [-1, 90, 13, 13]          32,400\n",
      "     BatchNorm2d-202           [-1, 90, 13, 13]             180\n",
      "       LeakyReLU-203           [-1, 90, 13, 13]               0\n",
      "          Conv2d-204           [-1, 90, 13, 13]          72,900\n",
      "     BatchNorm2d-205           [-1, 90, 13, 13]             180\n",
      "       LeakyReLU-206           [-1, 90, 13, 13]               0\n",
      "          Conv2d-207          [-1, 360, 13, 13]          32,400\n",
      "     BatchNorm2d-208          [-1, 360, 13, 13]             720\n",
      "       LeakyReLU-209          [-1, 360, 13, 13]               0\n",
      "      Bottleneck-210          [-1, 360, 13, 13]               0\n",
      "          Conv2d-211           [-1, 90, 13, 13]          32,400\n",
      "     BatchNorm2d-212           [-1, 90, 13, 13]             180\n",
      "       LeakyReLU-213           [-1, 90, 13, 13]               0\n",
      "          Conv2d-214           [-1, 90, 13, 13]          72,900\n",
      "     BatchNorm2d-215           [-1, 90, 13, 13]             180\n",
      "       LeakyReLU-216           [-1, 90, 13, 13]               0\n",
      "          Conv2d-217          [-1, 360, 13, 13]          32,400\n",
      "     BatchNorm2d-218          [-1, 360, 13, 13]             720\n",
      "       LeakyReLU-219          [-1, 360, 13, 13]               0\n",
      "      Bottleneck-220          [-1, 360, 13, 13]               0\n",
      "          Conv2d-221           [-1, 90, 13, 13]          32,400\n",
      "     BatchNorm2d-222           [-1, 90, 13, 13]             180\n",
      "       LeakyReLU-223           [-1, 90, 13, 13]               0\n",
      "          Conv2d-224           [-1, 90, 13, 13]          72,900\n",
      "     BatchNorm2d-225           [-1, 90, 13, 13]             180\n",
      "       LeakyReLU-226           [-1, 90, 13, 13]               0\n",
      "          Conv2d-227          [-1, 360, 13, 13]          32,400\n",
      "     BatchNorm2d-228          [-1, 360, 13, 13]             720\n",
      "       LeakyReLU-229          [-1, 360, 13, 13]               0\n",
      "      Bottleneck-230          [-1, 360, 13, 13]               0\n",
      "          Conv2d-231           [-1, 90, 13, 13]          32,400\n",
      "     BatchNorm2d-232           [-1, 90, 13, 13]             180\n",
      "       LeakyReLU-233           [-1, 90, 13, 13]               0\n",
      "          Conv2d-234           [-1, 90, 13, 13]          72,900\n",
      "     BatchNorm2d-235           [-1, 90, 13, 13]             180\n",
      "       LeakyReLU-236           [-1, 90, 13, 13]               0\n",
      "          Conv2d-237          [-1, 360, 13, 13]          32,400\n",
      "     BatchNorm2d-238          [-1, 360, 13, 13]             720\n",
      "       LeakyReLU-239          [-1, 360, 13, 13]               0\n",
      "      Bottleneck-240          [-1, 360, 13, 13]               0\n",
      "          Conv2d-241           [-1, 90, 13, 13]          32,400\n",
      "     BatchNorm2d-242           [-1, 90, 13, 13]             180\n",
      "       LeakyReLU-243           [-1, 90, 13, 13]               0\n",
      "          Conv2d-244           [-1, 90, 13, 13]          72,900\n",
      "     BatchNorm2d-245           [-1, 90, 13, 13]             180\n",
      "       LeakyReLU-246           [-1, 90, 13, 13]               0\n",
      "          Conv2d-247          [-1, 360, 13, 13]          32,400\n",
      "     BatchNorm2d-248          [-1, 360, 13, 13]             720\n",
      "       LeakyReLU-249          [-1, 360, 13, 13]               0\n",
      "      Bottleneck-250          [-1, 360, 13, 13]               0\n",
      "AdaptiveAvgPool2d-251            [-1, 360, 1, 1]               0\n",
      "          Linear-252                   [-1, 10]           3,610\n",
      "================================================================\n",
      "Total params: 1,804,826\n",
      "Trainable params: 1,804,826\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.11\n",
      "Forward/backward pass size (MB): 196.83\n",
      "Params size (MB): 6.88\n",
      "Estimated Total Size (MB): 203.82\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model,(3,96,96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/11/23 19:54:22\n",
      "epoch: 1/200 | trn loss: 2.0854 | val loss: 1.9136 | val accuracy: 27.5741% \n",
      "\n",
      "2020/11/23 19:56:32\n",
      "epoch: 2/200 | trn loss: 1.7898 | val loss: 1.7528 | val accuracy: 34.6655% \n",
      "\n",
      "2020/11/23 19:58:42\n",
      "epoch: 3/200 | trn loss: 1.6593 | val loss: 1.6011 | val accuracy: 40.7652% \n",
      "\n",
      "2020/11/23 20:00:52\n",
      "epoch: 4/200 | trn loss: 1.5569 | val loss: 1.5144 | val accuracy: 43.9153% \n",
      "\n",
      "2020/11/23 20:03:01\n",
      "epoch: 5/200 | trn loss: 1.4943 | val loss: 1.4834 | val accuracy: 46.1238% \n",
      "\n",
      "2020/11/23 20:05:09\n",
      "epoch: 6/200 | trn loss: 1.4366 | val loss: 1.4643 | val accuracy: 47.3057% \n",
      "\n",
      "2020/11/23 20:07:17\n",
      "epoch: 7/200 | trn loss: 1.3891 | val loss: 1.4044 | val accuracy: 47.9768% \n",
      "\n",
      "2020/11/23 20:09:25\n",
      "epoch: 8/200 | trn loss: 1.3451 | val loss: 1.3538 | val accuracy: 50.9215% \n",
      "\n",
      "2020/11/23 20:11:33\n",
      "epoch: 9/200 | trn loss: 1.3126 | val loss: 1.3810 | val accuracy: 49.9950% \n",
      "\n",
      "2020/11/23 20:13:41\n",
      "epoch: 10/200 | trn loss: 1.2877 | val loss: 1.3445 | val accuracy: 51.0266% \n",
      "\n",
      "2020/11/23 20:15:51\n",
      "epoch: 11/200 | trn loss: 1.2610 | val loss: 1.3219 | val accuracy: 52.2035% \n",
      "\n",
      "Model replaced and saved as  /home/jupyter-deeplearning/res_modelCustom_model_2.0_1.322\n",
      "2020/11/23 20:17:59\n",
      "epoch: 12/200 | trn loss: 1.2392 | val loss: 1.2964 | val accuracy: 52.5691% \n",
      "\n",
      "Model replaced and saved as  /home/jupyter-deeplearning/res_modelCustom_model_2.0_1.296\n",
      "2020/11/23 20:20:08\n",
      "epoch: 13/200 | trn loss: 1.2138 | val loss: 1.2975 | val accuracy: 53.5156% \n",
      "\n",
      "2020/11/23 20:22:16\n",
      "epoch: 14/200 | trn loss: 1.2045 | val loss: 1.2717 | val accuracy: 53.7810% \n",
      "\n",
      "Model replaced and saved as  /home/jupyter-deeplearning/res_modelCustom_model_2.0_1.272\n",
      "2020/11/23 20:24:24\n",
      "epoch: 15/200 | trn loss: 1.1917 | val loss: 1.2630 | val accuracy: 54.4221% \n",
      "\n",
      "Model replaced and saved as  /home/jupyter-deeplearning/res_modelCustom_model_2.0_1.263\n",
      "2020/11/23 20:26:32\n",
      "epoch: 16/200 | trn loss: 1.1810 | val loss: 1.2800 | val accuracy: 53.7159% \n",
      "\n",
      "2020/11/23 20:28:39\n",
      "epoch: 17/200 | trn loss: 1.1739 | val loss: 1.2168 | val accuracy: 56.2700% \n",
      "\n",
      "Model replaced and saved as  /home/jupyter-deeplearning/res_modelCustom_model_2.0_1.217\n",
      "2020/11/23 20:30:47\n",
      "epoch: 18/200 | trn loss: 1.1608 | val loss: 1.2364 | val accuracy: 55.7542% \n",
      "\n",
      "2020/11/23 20:32:55\n",
      "epoch: 19/200 | trn loss: 1.1561 | val loss: 1.2250 | val accuracy: 56.0146% \n",
      "\n",
      "2020/11/23 20:35:03\n",
      "epoch: 20/200 | trn loss: 1.1458 | val loss: 1.2208 | val accuracy: 56.4052% \n",
      "\n",
      "2020/11/23 20:37:11\n",
      "epoch: 21/200 | trn loss: 1.1347 | val loss: 1.2083 | val accuracy: 56.9561% \n",
      "\n",
      "Model replaced and saved as  /home/jupyter-deeplearning/res_modelCustom_model_2.0_1.208\n",
      "2020/11/23 20:39:20\n",
      "epoch: 22/200 | trn loss: 1.1275 | val loss: 1.2085 | val accuracy: 57.3918% \n",
      "\n",
      "2020/11/23 20:41:29\n",
      "epoch: 23/200 | trn loss: 1.1146 | val loss: 1.1950 | val accuracy: 56.4704% \n",
      "\n",
      "Model replaced and saved as  /home/jupyter-deeplearning/res_modelCustom_model_2.0_1.195\n",
      "2020/11/23 20:43:36\n",
      "epoch: 24/200 | trn loss: 1.1117 | val loss: 1.2121 | val accuracy: 56.7057% \n",
      "\n",
      "2020/11/23 20:45:44\n",
      "epoch: 25/200 | trn loss: 1.1109 | val loss: 1.1749 | val accuracy: 57.7324% \n",
      "\n",
      "Model replaced and saved as  /home/jupyter-deeplearning/res_modelCustom_model_2.0_1.175\n",
      "2020/11/23 20:47:50\n",
      "epoch: 26/200 | trn loss: 1.1100 | val loss: 1.2074 | val accuracy: 56.8209% \n",
      "\n",
      "2020/11/23 20:49:58\n",
      "epoch: 27/200 | trn loss: 1.1012 | val loss: 1.2102 | val accuracy: 57.0162% \n",
      "\n",
      "2020/11/23 20:52:05\n",
      "epoch: 28/200 | trn loss: 1.0916 | val loss: 1.1952 | val accuracy: 57.9778% \n",
      "\n",
      "2020/11/23 20:54:12\n",
      "epoch: 29/200 | trn loss: 1.0911 | val loss: 1.1716 | val accuracy: 57.8225% \n",
      "\n",
      "Model replaced and saved as  /home/jupyter-deeplearning/res_modelCustom_model_2.0_1.172\n",
      "2020/11/23 20:56:21\n",
      "epoch: 30/200 | trn loss: 1.0946 | val loss: 1.1876 | val accuracy: 57.4820% \n",
      "\n",
      "2020/11/23 20:58:29\n",
      "epoch: 31/200 | trn loss: 0.9477 | val loss: 1.0432 | val accuracy: 62.5401% \n",
      "\n",
      "Model replaced and saved as  /home/jupyter-deeplearning/res_modelCustom_model_2.0_1.043\n",
      "2020/11/23 21:00:37\n",
      "epoch: 32/200 | trn loss: 0.9101 | val loss: 1.0425 | val accuracy: 62.6753% \n",
      "\n",
      "Model replaced and saved as  /home/jupyter-deeplearning/res_modelCustom_model_2.0_1.043\n",
      "2020/11/23 21:02:46\n",
      "epoch: 33/200 | trn loss: 0.8958 | val loss: 1.0137 | val accuracy: 63.7320% \n",
      "\n",
      "Model replaced and saved as  /home/jupyter-deeplearning/res_modelCustom_model_2.0_1.014\n",
      "2020/11/23 21:04:54\n",
      "epoch: 34/200 | trn loss: 0.8795 | val loss: 1.0338 | val accuracy: 63.1811% \n",
      "\n",
      "2020/11/23 21:07:03\n",
      "epoch: 35/200 | trn loss: 0.8691 | val loss: 1.0204 | val accuracy: 63.5367% \n",
      "\n",
      "2020/11/23 21:09:10\n",
      "epoch: 36/200 | trn loss: 0.8618 | val loss: 1.0250 | val accuracy: 63.5367% \n",
      "\n",
      "2020/11/23 21:11:18\n",
      "epoch: 37/200 | trn loss: 0.8493 | val loss: 1.0237 | val accuracy: 63.4014% \n",
      "\n",
      "2020/11/23 21:13:25\n",
      "epoch: 38/200 | trn loss: 0.8443 | val loss: 1.0085 | val accuracy: 63.9473% \n",
      "\n",
      "Model replaced and saved as  /home/jupyter-deeplearning/res_modelCustom_model_2.0_1.008\n",
      "2020/11/23 21:15:33\n",
      "epoch: 39/200 | trn loss: 0.8405 | val loss: 1.0135 | val accuracy: 64.2428% \n",
      "\n",
      "2020/11/23 21:17:41\n",
      "epoch: 40/200 | trn loss: 0.8324 | val loss: 1.0280 | val accuracy: 63.4265% \n",
      "\n",
      "2020/11/23 21:19:49\n",
      "epoch: 41/200 | trn loss: 0.8284 | val loss: 1.0035 | val accuracy: 64.5032% \n",
      "\n",
      "Model replaced and saved as  /home/jupyter-deeplearning/res_modelCustom_model_2.0_1.004\n",
      "2020/11/23 21:21:57\n",
      "epoch: 42/200 | trn loss: 0.8140 | val loss: 1.0315 | val accuracy: 63.8972% \n",
      "\n",
      "2020/11/23 21:24:04\n",
      "epoch: 43/200 | trn loss: 0.8103 | val loss: 1.0173 | val accuracy: 64.1727% \n",
      "\n",
      "2020/11/23 21:26:13\n",
      "epoch: 44/200 | trn loss: 0.8052 | val loss: 1.0113 | val accuracy: 64.0875% \n",
      "\n",
      "2020/11/23 21:28:21\n",
      "epoch: 45/200 | trn loss: 0.8036 | val loss: 1.0080 | val accuracy: 64.1126% \n",
      "\n",
      "2020/11/23 21:30:29\n",
      "epoch: 46/200 | trn loss: 0.7945 | val loss: 1.0268 | val accuracy: 64.0475% \n",
      "\n",
      "2020/11/23 21:32:37\n",
      "epoch: 47/200 | trn loss: 0.7882 | val loss: 1.0177 | val accuracy: 64.3580% \n",
      "\n",
      "2020/11/23 21:34:44\n",
      "epoch: 48/200 | trn loss: 0.7887 | val loss: 1.0326 | val accuracy: 63.9824% \n",
      "\n",
      "2020/11/23 21:36:50\n",
      "epoch: 49/200 | trn loss: 0.7842 | val loss: 1.0096 | val accuracy: 64.6134% \n",
      "\n",
      "2020/11/23 21:38:59\n",
      "epoch: 50/200 | trn loss: 0.7756 | val loss: 1.0078 | val accuracy: 64.2228% \n",
      "\n",
      "2020/11/23 21:41:07\n",
      "epoch: 51/200 | trn loss: 0.7742 | val loss: 1.0061 | val accuracy: 64.5483% \n",
      "\n",
      "2020/11/23 21:43:15\n",
      "epoch: 52/200 | trn loss: 0.7685 | val loss: 1.0168 | val accuracy: 64.3880% \n",
      "\n",
      "2020/11/23 21:45:21\n",
      "epoch: 53/200 | trn loss: 0.7719 | val loss: 1.0237 | val accuracy: 64.1577% \n",
      "\n",
      "2020/11/23 21:47:28\n",
      "epoch: 54/200 | trn loss: 0.7622 | val loss: 1.0177 | val accuracy: 63.9473% \n",
      "\n",
      "2020/11/23 21:49:36\n",
      "epoch: 55/200 | trn loss: 0.7580 | val loss: 0.9949 | val accuracy: 65.2043% \n",
      "\n",
      "Model replaced and saved as  /home/jupyter-deeplearning/res_modelCustom_model_2.0_0.995\n",
      "2020/11/23 21:51:43\n",
      "epoch: 56/200 | trn loss: 0.7583 | val loss: 1.0130 | val accuracy: 64.5533% \n",
      "\n",
      "2020/11/23 21:53:50\n",
      "epoch: 57/200 | trn loss: 0.7510 | val loss: 1.0221 | val accuracy: 64.5533% \n",
      "\n",
      "2020/11/23 21:55:57\n",
      "epoch: 58/200 | trn loss: 0.7480 | val loss: 1.0174 | val accuracy: 64.7085% \n",
      "\n",
      "2020/11/23 21:58:05\n",
      "epoch: 59/200 | trn loss: 0.7438 | val loss: 1.0283 | val accuracy: 64.3930% \n",
      "\n",
      "2020/11/23 22:00:12\n",
      "epoch: 60/200 | trn loss: 0.7441 | val loss: 1.0208 | val accuracy: 64.3029% \n",
      "\n",
      "2020/11/23 22:02:20\n",
      "epoch: 61/200 | trn loss: 0.7042 | val loss: 0.9865 | val accuracy: 65.7151% \n",
      "\n",
      "Model replaced and saved as  /home/jupyter-deeplearning/res_modelCustom_model_2.0_0.987\n",
      "2020/11/23 22:04:28\n",
      "epoch: 62/200 | trn loss: 0.6936 | val loss: 0.9959 | val accuracy: 65.5799% \n",
      "\n",
      "2020/11/23 22:06:35\n",
      "epoch: 63/200 | trn loss: 0.6948 | val loss: 0.9808 | val accuracy: 65.7652% \n",
      "\n",
      "Model replaced and saved as  /home/jupyter-deeplearning/res_modelCustom_model_2.0_0.981\n",
      "2020/11/23 22:08:43\n",
      "epoch: 64/200 | trn loss: 0.6865 | val loss: 0.9857 | val accuracy: 66.0407% \n",
      "\n",
      "2020/11/23 22:10:50\n",
      "epoch: 65/200 | trn loss: 0.6882 | val loss: 0.9875 | val accuracy: 65.8704% \n",
      "\n",
      "2020/11/23 22:12:56\n",
      "epoch: 66/200 | trn loss: 0.6888 | val loss: 0.9838 | val accuracy: 65.6500% \n",
      "\n",
      "2020/11/23 22:15:03\n",
      "epoch: 67/200 | trn loss: 0.6818 | val loss: 0.9882 | val accuracy: 65.6601% \n",
      "\n",
      "2020/11/23 22:17:09\n",
      "epoch: 68/200 | trn loss: 0.6828 | val loss: 0.9915 | val accuracy: 65.9555% \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/11/23 22:19:16\n",
      "epoch: 69/200 | trn loss: 0.6839 | val loss: 0.9866 | val accuracy: 65.8403% \n",
      "\n",
      "2020/11/23 22:21:22\n",
      "epoch: 70/200 | trn loss: 0.6821 | val loss: 1.0008 | val accuracy: 65.2344% \n",
      "\n",
      "2020/11/23 22:23:28\n",
      "epoch: 71/200 | trn loss: 0.6811 | val loss: 0.9853 | val accuracy: 66.0407% \n",
      "\n",
      "2020/11/23 22:25:34\n",
      "epoch: 72/200 | trn loss: 0.6823 | val loss: 0.9838 | val accuracy: 66.2961% \n",
      "\n",
      "2020/11/23 22:27:42\n",
      "epoch: 73/200 | trn loss: 0.6801 | val loss: 0.9933 | val accuracy: 66.0507% \n",
      "\n",
      "2020/11/23 22:29:48\n",
      "epoch: 74/200 | trn loss: 0.6740 | val loss: 0.9913 | val accuracy: 65.7552% \n",
      "\n",
      "2020/11/23 22:31:54\n",
      "epoch: 75/200 | trn loss: 0.6769 | val loss: 0.9881 | val accuracy: 66.0256% \n",
      "\n",
      "2020/11/23 22:33:59\n",
      "epoch: 76/200 | trn loss: 0.6765 | val loss: 0.9799 | val accuracy: 66.1859% \n",
      "\n",
      "Model replaced and saved as  /home/jupyter-deeplearning/res_modelCustom_model_2.0_0.980\n",
      "2020/11/23 22:36:06\n",
      "epoch: 77/200 | trn loss: 0.6730 | val loss: 0.9945 | val accuracy: 65.4397% \n",
      "\n",
      "2020/11/23 22:38:12\n",
      "epoch: 78/200 | trn loss: 0.6723 | val loss: 0.9974 | val accuracy: 65.8203% \n",
      "\n",
      "2020/11/23 22:40:19\n",
      "epoch: 79/200 | trn loss: 0.6718 | val loss: 0.9918 | val accuracy: 66.0707% \n",
      "\n",
      "2020/11/23 22:42:25\n",
      "epoch: 80/200 | trn loss: 0.6716 | val loss: 0.9869 | val accuracy: 65.8003% \n",
      "\n",
      "2020/11/23 22:44:32\n",
      "epoch: 81/200 | trn loss: 0.6688 | val loss: 0.9936 | val accuracy: 65.8654% \n",
      "\n",
      "2020/11/23 22:46:35\n",
      "epoch: 82/200 | trn loss: 0.6669 | val loss: 0.9919 | val accuracy: 65.9004% \n",
      "\n",
      "2020/11/23 22:48:38\n",
      "epoch: 83/200 | trn loss: 0.6664 | val loss: 1.0081 | val accuracy: 65.5048% \n",
      "\n"
     ]
    }
   ],
   "source": [
    "saving_path=\"/home/jupyter-deeplearning/res_model\"\n",
    "trn_loss_list = []\n",
    "val_loss_list = []\n",
    "val_acc_list = []\n",
    "total_epoch=200\n",
    "model_char=\"2.0\"\n",
    "model_name=\"\"\n",
    "patience=10\n",
    "start_early_stop_check=0\n",
    "saving_start_epoch=10\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    trn_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        if torch.cuda.is_available():\n",
    "            inputs=inputs.cuda()\n",
    "            labels=labels.cuda()\n",
    "        # grad init\n",
    "        optimizer.zero_grad()\n",
    "        # forward propagation\n",
    "        output= model(inputs)\n",
    "        # calculate loss\n",
    "        loss=criterion(output, labels)\n",
    "        # back propagation \n",
    "        loss.backward()\n",
    "        # weight update\n",
    "        optimizer.step()\n",
    "        \n",
    "        # trn_loss summary\n",
    "        trn_loss += loss.item()\n",
    "        # del (memory issue)\n",
    "        del loss\n",
    "        del output\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        cor_match = 0\n",
    "        for j, val in enumerate(test_loader):\n",
    "            val_x, val_label = val\n",
    "            if torch.cuda.is_available():\n",
    "                val_x = val_x.cuda()\n",
    "                val_label =val_label.cuda()\n",
    "            val_output = model(val_x)\n",
    "            v_loss = criterion(val_output, val_label)\n",
    "            val_loss += v_loss\n",
    "            _, predicted=torch.max(val_output,1)\n",
    "            cor_match+=np.count_nonzero(predicted.cpu().detach()==val_label.cpu().detach())\n",
    "    del val_output\n",
    "    del v_loss\n",
    "    del predicted\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    \n",
    "    \n",
    "    trn_loss_list.append(trn_loss/len(train_loader))\n",
    "    val_loss_list.append(val_loss/len(test_loader))\n",
    "    val_acc=cor_match/(len(test_loader)*batch_size)\n",
    "    val_acc_list.append(val_acc)\n",
    "    now = time.localtime()\n",
    "    print (\"%04d/%02d/%02d %02d:%02d:%02d\" % (now.tm_year, now.tm_mon, now.tm_mday, now.tm_hour, now.tm_min, now.tm_sec))\n",
    "\n",
    "    print(\"epoch: {}/{} | trn loss: {:.4f} | val loss: {:.4f} | val accuracy: {:.4f}% \\n\".format(\n",
    "                epoch+1, total_epoch, trn_loss / len(train_loader), val_loss / len(test_loader), val_acc*100\n",
    "            ))\n",
    "    \n",
    "    \n",
    "    if epoch+1>2:\n",
    "        if val_loss_list[-1]>val_loss_list[-2]:\n",
    "            start_early_stop_check=1\n",
    "    else:\n",
    "        val_loss_min=val_loss_list[-1]\n",
    "        \n",
    "    if start_early_stop_check:\n",
    "        early_stop_temp=val_loss_list[-patience:]\n",
    "        if all(early_stop_temp[i]<early_stop_temp[i+1] for i in range (len(early_stop_temp)-1)):\n",
    "            print(\"Early stop!\")\n",
    "            break\n",
    "            \n",
    "    if epoch+1>saving_start_epoch:\n",
    "        if val_loss_list[-1]<val_loss_min:\n",
    "            if os.path.isfile(model_name):\n",
    "                os.remove(model_name)\n",
    "            val_loss_min=val_loss_list[-1]\n",
    "            model_name=saving_path+\"Custom_model_\"+model_char+\"_{:.3f}\".format(val_loss_min)\n",
    "            torch.save(model, model_name)\n",
    "            print(\"Model replaced and saved as \",model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder(\n",
    "        train_path_path,\n",
    "        transforms.Compose([\n",
    "        transforms.RandomRotation(degrees=45),\n",
    "        transforms.Resize(120),    \n",
    "        transforms.ColorJitter(.3,.3,.3,.3),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "for _ in range (19):\n",
    "    aug_data = datasets.ImageFolder(\n",
    "        train_path_path,\n",
    "        transforms.Compose([\n",
    "        transforms.RandomRotation(degrees=45),\n",
    "        transforms.Resize(120),    \n",
    "        transforms.ColorJitter(.3,.3,.3,.3),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "    dataset=torch.utils.data.ConcatDataset([dataset,aug_data])\n",
    "valset = datasets.ImageFolder(\n",
    "        test_path_path,\n",
    "        transforms.Compose([\n",
    "        transforms.RandomRotation(degrees=45),\n",
    "        transforms.Resize(120),\n",
    "        transforms.ColorJitter(.3,.3,.3,.3),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "for _ in range (19):\n",
    "    aug_data = datasets.ImageFolder(\n",
    "        test_path_path,\n",
    "        transforms.Compose([\n",
    "        transforms.RandomRotation(degrees=45),\n",
    "        transforms.Resize(120),    \n",
    "        transforms.ColorJitter(.3,.3,.3,.3),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "    valset=torch.utils.data.ConcatDataset([valset,aug_data])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=True,\n",
    "        num_workers=8, pin_memory=True,drop_last=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        valset, batch_size=batch_size, shuffle=True,\n",
    "        num_workers=8, pin_memory=True,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fine tuning\n",
    "\n",
    "total_epoch=80\n",
    "model_char=\"2.0\"\n",
    "model_name=\"\"\n",
    "patience=10\n",
    "start_early_stop_check=0\n",
    "saving_start_epoch=10\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    trn_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        if torch.cuda.is_available():\n",
    "            inputs=inputs.cuda()\n",
    "            labels=labels.cuda()\n",
    "        # grad init\n",
    "        optimizer.zero_grad()\n",
    "        # forward propagation\n",
    "        output= model(inputs)\n",
    "        # calculate loss\n",
    "        loss=criterion(output, labels)\n",
    "        # back propagation \n",
    "        loss.backward()\n",
    "        # weight update\n",
    "        optimizer.step()\n",
    "        \n",
    "        # trn_loss summary\n",
    "        trn_loss += loss.item()\n",
    "        # del (memory issue)\n",
    "        del loss\n",
    "        del output\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        cor_match = 0\n",
    "        for j, val in enumerate(test_loader):\n",
    "            val_x, val_label = val\n",
    "            if torch.cuda.is_available():\n",
    "                val_x = val_x.cuda()\n",
    "                val_label =val_label.cuda()\n",
    "            val_output = model(val_x)\n",
    "            v_loss = criterion(val_output, val_label)\n",
    "            val_loss += v_loss\n",
    "            _, predicted=torch.max(val_output,1)\n",
    "            cor_match+=np.count_nonzero(predicted.cpu().detach()==val_label.cpu().detach())\n",
    "    del val_output\n",
    "    del v_loss\n",
    "    del predicted\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    \n",
    "    \n",
    "    trn_loss_list.append(trn_loss/len(train_loader))\n",
    "    val_loss_list.append(val_loss/len(test_loader))\n",
    "    val_acc=cor_match/(len(test_loader)*batch_size)\n",
    "    val_acc_list.append(val_acc)\n",
    "    now = time.localtime()\n",
    "    print (\"%04d/%02d/%02d %02d:%02d:%02d\" % (now.tm_year, now.tm_mon, now.tm_mday, now.tm_hour, now.tm_min, now.tm_sec))\n",
    "\n",
    "    print(\"epoch: {}/{} | trn loss: {:.4f} | val loss: {:.4f} | val accuracy: {:.4f}% \\n\".format(\n",
    "                epoch+1, total_epoch, trn_loss / len(train_loader), val_loss / len(test_loader), val_acc*100\n",
    "            ))\n",
    "    \n",
    "    \n",
    "    if epoch+1>2:\n",
    "        if val_loss_list[-1]>val_loss_list[-2]:\n",
    "            start_early_stop_check=1\n",
    "    else:\n",
    "        val_loss_min=val_loss_list[-1]\n",
    "        \n",
    "    if start_early_stop_check:\n",
    "        early_stop_temp=val_loss_list[-patience:]\n",
    "        if all(early_stop_temp[i]<early_stop_temp[i+1] for i in range (len(early_stop_temp)-1)):\n",
    "            print(\"Early stop!\")\n",
    "            break\n",
    "            \n",
    "    if epoch+1>saving_start_epoch:\n",
    "        if val_loss_list[-1]<val_loss_min:\n",
    "            if os.path.isfile(model_name):\n",
    "                os.remove(model_name)\n",
    "            val_loss_min=val_loss_list[-1]\n",
    "            model_name=\"Custom_model_\"+model_char+\"_{:.3f}\".format(val_loss_min)\n",
    "            torch.save(model, model_name)\n",
    "            print(\"Model replaced and saved as \",model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plt.ylabel(\"val_accuracy\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(val_acc_list)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_acc_list=np.array(val_acc_list)\n",
    "# np.savetxt(\"ver_2.0.txt\", val_acc_list, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import pandas as pd\n",
    "# import argparse\n",
    "# import time\n",
    "# import torchvision\n",
    "# import torchvision.transforms as transforms\n",
    "# from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize = transforms.Normalize(mean=[0.507, 0.487, 0.441],\n",
    "#                                      std=[0.267, 0.256, 0.276])\n",
    "# test_transform = transforms.Compose([\n",
    "#         transforms.ToTensor(),\n",
    "#         normalize\n",
    "#     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset = torchvision.datasets.ImageFolder('./data/test', transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category = []\n",
    "# for input, _ in test_loader:\n",
    "#     input = input.cuda()\n",
    "#     output = model(input)\n",
    "#     output = torch.argmax(output, dim=1)\n",
    "#     Category = Category + output.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Id = list(range(0, 8000))\n",
    "# samples = {\n",
    "#    'Id': Id,\n",
    "#    'Category': Category \n",
    "# }\n",
    "# df = pd.DataFrame(samples, columns=['Id', 'Category'])\n",
    "\n",
    "# df.to_csv('submission_2.0_2.csv', index=False)\n",
    "# print('Done!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
