{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import torch.distributed as dist\n",
    "import math\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import Type, Any, Callable, Union, List, Optional\n",
    "from torch import Tensor\n",
    "from torch.hub import load_state_dict_from_url\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.data.sampler import BatchSampler\n",
    "from collections.abc import Mapping, Sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train path /home/jupyter-deeplearning/Organize/data/train/\n"
     ]
    }
   ],
   "source": [
    "train_path_path= f\"{os.getcwd()}/data/train/\"\n",
    "print(f\"train path {train_path_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "num_gpus=4\n",
    "num_workers=8\n",
    "lr=2e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input_size_Image=96\n",
    "Test_size=Input_size_Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RASampler(torch.utils.data.Sampler):\n",
    "    \"\"\"\n",
    "    Batch Sampler with Repeated Augmentations (RA)\n",
    "    - dataset_len: original length of the dataset\n",
    "    - batch_size\n",
    "    - repetitions: instances per image\n",
    "    - len_factor: multiplicative factor for epoch size\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,dataset,num_replicas, rank, dataset_len, batch_size, repetitions=1, len_factor=1.0, shuffle=False, drop_last=False):\n",
    "        self.dataset=dataset\n",
    "        self.dataset_len = dataset_len\n",
    "        self.batch_size = batch_size\n",
    "        self.repetitions = repetitions\n",
    "        self.len_images = int(dataset_len * len_factor)\n",
    "        self.shuffle = shuffle\n",
    "        self.drop_last = drop_last\n",
    "        if num_replicas is None:\n",
    "            if not dist.is_available():\n",
    "                raise RuntimeError(\"Requires distributed package to be available\")\n",
    "            num_replicas = dist.get_world_size()\n",
    "        if rank is None:\n",
    "            if not dist.is_available():\n",
    "                raise RuntimeError(\"Requires distributed package to be available\")\n",
    "            rank = dist.get_rank()\n",
    "        self.dataset = dataset\n",
    "        self.num_replicas = num_replicas\n",
    "        self.rank = rank\n",
    "        self.epoch = 0\n",
    "        self.num_samples = int(math.ceil(len(self.dataset) * self.repetitions * 1.0 / self.num_replicas))\n",
    "        self.total_size = self.num_samples * self.num_replicas\n",
    "        \n",
    "        \n",
    "    def shuffler(self):\n",
    "        if self.shuffle:\n",
    "            new_perm = lambda: iter(np.random.permutation(self.dataset_len))\n",
    "        else:\n",
    "            new_perm = lambda: iter(np.arange(self.dataset_len))\n",
    "        shuffle = new_perm()\n",
    "        while True:\n",
    "            try:\n",
    "                index = next(shuffle)\n",
    "            except StopIteration:\n",
    "                shuffle = new_perm()\n",
    "                index = next(shuffle)\n",
    "            for repetition in range(self.repetitions):\n",
    "                yield index\n",
    "\n",
    "    def __iter__(self):\n",
    "        shuffle = iter(self.shuffler())\n",
    "        seen = 0\n",
    "        indices=[]\n",
    "        for _ in range(self.len_images):\n",
    "            index = next(shuffle)\n",
    "            indices.append(index)\n",
    "        indices += indices[:(self.total_size - len(indices))]\n",
    "        assert len(indices) == self.total_size\n",
    "        # subsample\n",
    "        indices = indices[self.rank:self.total_size:self.num_replicas]\n",
    "        assert len(indices) == self.num_samples\n",
    "\n",
    "        return iter(indices)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def set_epoch(self, epoch):\n",
    "        self.epoch = epoch\n",
    "        \n",
    "def list_collate(batch):\n",
    "    \"\"\"\n",
    "    Collate into a list instead of a tensor to deal with variable-sized inputs\n",
    "    \"\"\"\n",
    "    elem_type = type(batch[0])\n",
    "    if isinstance(batch[0], torch.Tensor):\n",
    "        return batch\n",
    "    elif elem_type.__module__ == 'numpy':\n",
    "        if elem_type.__name__ == 'ndarray':\n",
    "            return list_collate([torch.from_numpy(b) for b in batch])\n",
    "    elif isinstance(batch[0], Mapping):\n",
    "        return {key: list_collate([d[key] for d in batch]) for key in batch[0]}\n",
    "    elif isinstance(batch[0], Sequence):\n",
    "        transposed = zip(*batch)\n",
    "        return [list_collate(samples) for samples in transposed]\n",
    "    return default_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resize(transforms.Resize):\n",
    "    \"\"\"\n",
    "    Resize with a ``largest=False'' argument\n",
    "    allowing to resize to a common largest side without cropping\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, size, largest=False, **kwargs):\n",
    "        super().__init__(size, **kwargs)\n",
    "        self.largest = largest\n",
    "\n",
    "    @staticmethod\n",
    "    def target_size(w, h, size, largest=False):\n",
    "        if h < w and largest:\n",
    "            w, h = size, int(size * h / w)\n",
    "        else:\n",
    "            w, h = int(size * w / h), size\n",
    "        size = (h, w)\n",
    "        return size\n",
    "\n",
    "    def __call__(self, img):\n",
    "        size = self.size\n",
    "        w, h = img.size\n",
    "        target_size = self.target_size(w, h, size, self.largest)\n",
    "        return F.resize(img, target_size, self.interpolation)\n",
    "\n",
    "    def __repr__(self):\n",
    "        r = super().__repr__()\n",
    "        return r[:-1] + ', largest={})'.format(self.largest)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_transforms(input_size=224,test_size=224, kind='full', crop=True, need=('train', 'val'), backbone=None):\n",
    "    mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "    if backbone is not None and backbone in ['pnasnet5large', 'nasnetamobile']:\n",
    "        mean, std = [0.5, 0.5, 0.5], [0.5, 0.5, 0.5]\n",
    "\n",
    "    transformations = {}\n",
    "    if 'train' in need:\n",
    "        if kind == 'torch':\n",
    "            transformations['train'] = transforms.Compose([\n",
    "                transforms.RandomResizedCrop(input_size),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std),\n",
    "            ])\n",
    "        elif kind == 'full':\n",
    "            transformations['train'] = transforms.Compose([\n",
    "                transforms.RandomResizedCrop(input_size),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ColorJitter(0.3, 0.3, 0.3),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std),\n",
    "            ])\n",
    "\n",
    "        else:\n",
    "            raise ValueError('Transforms kind {} unknown'.format(kind))\n",
    "    if 'val' in need:\n",
    "        if crop:\n",
    "            transformations['val'] = transforms.Compose(\n",
    "                [Resize(int((256 / 224) * test_size)),  # to maintain same ratio w.r.t. 224 images\n",
    "                 transforms.CenterCrop(test_size),\n",
    "                 transforms.ToTensor(),\n",
    "                 transforms.Normalize(mean, std)])\n",
    "        else:\n",
    "            transformations['val'] = transforms.Compose(\n",
    "                [Resize(test_size, largest=True),  \n",
    "                 transforms.ToTensor(),\n",
    "                 transforms.Normalize(mean, std)])\n",
    "    return transformations\n",
    "\n",
    "transforms_list = ['torch', 'full']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transf=get_transforms(input_size=Input_size_Image,test_size=Test_size, kind='full', crop=True, need=('train', 'val'), backbone=None)\n",
    "transform_train = transf['train']\n",
    "transform_test = transf['val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder(\n",
    "        train_path_path,\n",
    "        transform=transform_train)\n",
    "for _ in range (9):\n",
    "    aug_data = datasets.ImageFolder(\n",
    "        train_path_path,\n",
    "        transform=transform_train)\n",
    "    dataset=torch.utils.data.ConcatDataset([dataset,aug_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_sampler = RASampler(\n",
    "            train_dataset,num_workers,0,len(train_dataset),batch_size,repetitions=3,len_factor=2.0,shuffle=True, drop_last=False)\n",
    "        \n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset, batch_size=batch_size,\n",
    "            num_workers=num_workers, pin_memory=True,\n",
    "            sampler=train_sampler,drop_last=True\n",
    "        )\n",
    "    \n",
    "    \n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=batch_size, shuffle=True,\n",
    "        num_workers=num_workers, pin_memory=True,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_urls = {\n",
    "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
    "    'resnext50_32x4d': 'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth',\n",
    "    'resnext101_32x8d': 'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth',\n",
    "    'wide_resnet50_2': 'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth',\n",
    "    'wide_resnet101_2': 'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth',\n",
    "    'resnext101_32x8d': 'https://download.pytorch.org/models/ig_resnext101_32x8-c38310e5.pth',\n",
    "    'resnext101_32x16d': 'https://download.pytorch.org/models/ig_resnext101_32x16-c6f796b0.pth',\n",
    "    'resnext101_32x32d': 'https://download.pytorch.org/models/ig_resnext101_32x32-e4b90b00.pth',\n",
    "    'resnext101_32x48d': 'https://download.pytorch.org/models/ig_resnext101_32x48-3e41cc8a.pth',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
    "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
    "    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n",
    "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
    "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
    "\n",
    "    expansion: int = 4\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: Optional[nn.Module] = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
    "    ) -> None:\n",
    "        super(Bottleneck, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        width = int(planes * (base_width / 64.)) * groups\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        block: Type[Union[Bottleneck]],\n",
    "        layers: List[int],\n",
    "        num_classes: int = 10,\n",
    "        zero_init_residual: bool = False,\n",
    "        groups: int = 1,\n",
    "        width_per_group: int = 64,\n",
    "        replace_stride_with_dilation: Optional[List[bool]] = None,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
    "    ) -> None:\n",
    "        super(ResNet, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
    "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]\n",
    "\n",
    "    def _make_layer(self, block: Type[Union[Bottleneck]], planes: int, blocks: int,\n",
    "                    stride: int = 1, dilate: bool = False) -> nn.Sequential:\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "                            self.base_width, previous_dilation, norm_layer))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation,\n",
    "                                norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
    "        # See note [TorchScript super()]\n",
    "        x = x.view(batch_size//num_gpus,3,96,96)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(batch_size//num_gpus,-1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self._forward_impl(x)\n",
    "\n",
    "\n",
    "def _resnet(\n",
    "    arch: str,\n",
    "    block: Type[Union[Bottleneck]],\n",
    "    layers: List[int],\n",
    "    pretrained: bool,\n",
    "    progress: bool,\n",
    "    **kwargs: Any\n",
    ") -> ResNet:\n",
    "    model = ResNet(block, layers, **kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
    "                                              progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "def _resnext(arch, block, layers, pretrained, progress, **kwargs):\n",
    "    model = ResNet(block, layers, **kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = load_state_dict_from_url(model_urls[arch], progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnext101_32x16d_wsl(pretrained: bool = False, progress: bool = True, **kwargs):\n",
    "    \"\"\"Constructs a ResNeXt-101 32x16 model pre-trained on weakly-supervised data\n",
    "    and finetuned on ImageNet from Figure 5 in\n",
    "    `\"Exploring the Limits of Weakly Supervised Pretraining\" <https://arxiv.org/abs/1805.00932>`_\n",
    "    Args:\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr.\n",
    "    \"\"\"\n",
    "    kwargs['groups'] = 32\n",
    "    kwargs['width_per_group'] = 16\n",
    "    return _resnext('resnext101_32x16d', Bottleneck, [3, 4, 23, 3], pretrained, progress, **kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnext101_32x16d_wsl()\n",
    "model=nn.DataParallel(model.cuda())\n",
    "criterion = nn.CrossEntropyLoss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_scaled_lr = 8.0 * lr * batch_size * num_workers /512.0\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=linear_scaled_lr, momentum=0.9,weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-deeplearning/res_model\n"
     ]
    }
   ],
   "source": [
    "%cd /home/jupyter-deeplearning/res_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/11/01 17:44:52\n",
      "epoch: 1/150 | trn loss: 3.6085 | val loss: 2.3069 | val accuracy: 10.0160% \n",
      "\n",
      "2020/11/01 17:46:49\n",
      "epoch: 2/150 | trn loss: 2.3029 | val loss: 2.2884 | val accuracy: 13.2512% \n",
      "\n",
      "2020/11/01 17:48:45\n",
      "epoch: 3/150 | trn loss: 2.2669 | val loss: 2.1945 | val accuracy: 16.9371% \n",
      "\n",
      "2020/11/01 17:50:40\n",
      "epoch: 4/150 | trn loss: 2.1434 | val loss: 2.0849 | val accuracy: 19.7216% \n",
      "\n",
      "2020/11/01 17:52:36\n",
      "epoch: 5/150 | trn loss: 2.0371 | val loss: 2.0056 | val accuracy: 21.9451% \n",
      "\n",
      "2020/11/01 17:54:31\n",
      "epoch: 6/150 | trn loss: 1.9527 | val loss: 1.9586 | val accuracy: 24.0785% \n",
      "\n",
      "2020/11/01 17:56:27\n",
      "epoch: 7/150 | trn loss: 1.8994 | val loss: 1.8655 | val accuracy: 25.8313% \n",
      "\n",
      "2020/11/01 17:58:23\n",
      "epoch: 8/150 | trn loss: 1.8745 | val loss: 1.8426 | val accuracy: 28.9864% \n",
      "\n",
      "2020/11/01 18:00:18\n",
      "epoch: 9/150 | trn loss: 1.8573 | val loss: 1.8289 | val accuracy: 28.9263% \n",
      "\n",
      "2020/11/01 18:02:14\n",
      "epoch: 10/150 | trn loss: 1.8230 | val loss: 1.7944 | val accuracy: 31.6406% \n",
      "\n",
      "2020/11/01 18:04:10\n",
      "epoch: 11/150 | trn loss: 1.8069 | val loss: 1.7757 | val accuracy: 32.2716% \n",
      "\n",
      "Model replaced and saved as  Custom_model_1.9_1.776\n",
      "2020/11/01 18:06:07\n",
      "epoch: 12/150 | trn loss: 1.7746 | val loss: 1.7857 | val accuracy: 31.9912% \n",
      "\n",
      "2020/11/01 18:08:04\n",
      "epoch: 13/150 | trn loss: 1.7559 | val loss: 1.7364 | val accuracy: 34.2548% \n",
      "\n",
      "Model replaced and saved as  Custom_model_1.9_1.736\n",
      "2020/11/01 18:12:51\n",
      "epoch: 14/150 | trn loss: 1.7604 | val loss: 1.7295 | val accuracy: 35.1362% \n",
      "\n",
      "Model replaced and saved as  Custom_model_1.9_1.729\n",
      "2020/11/01 18:17:59\n",
      "epoch: 15/150 | trn loss: 1.7346 | val loss: 1.7256 | val accuracy: 34.8057% \n",
      "\n",
      "Model replaced and saved as  Custom_model_1.9_1.726\n",
      "2020/11/01 18:21:01\n",
      "epoch: 16/150 | trn loss: 1.6990 | val loss: 1.6718 | val accuracy: 37.0793% \n",
      "\n",
      "Model replaced and saved as  Custom_model_1.9_1.672\n",
      "2020/11/01 18:23:00\n",
      "epoch: 17/150 | trn loss: 1.6686 | val loss: 1.6266 | val accuracy: 38.8722% \n",
      "\n",
      "Model replaced and saved as  Custom_model_1.9_1.627\n",
      "2020/11/01 18:24:58\n",
      "epoch: 18/150 | trn loss: 1.6320 | val loss: 1.6753 | val accuracy: 37.1394% \n",
      "\n",
      "2020/11/01 18:26:53\n",
      "epoch: 19/150 | trn loss: 1.6165 | val loss: 1.5808 | val accuracy: 41.5565% \n",
      "\n",
      "Model replaced and saved as  Custom_model_1.9_1.581\n",
      "2020/11/01 18:28:50\n",
      "epoch: 20/150 | trn loss: 1.6207 | val loss: 1.5601 | val accuracy: 41.7869% \n",
      "\n",
      "Model replaced and saved as  Custom_model_1.9_1.560\n",
      "2020/11/01 18:30:48\n",
      "epoch: 21/150 | trn loss: 1.5594 | val loss: 1.5777 | val accuracy: 41.1959% \n",
      "\n",
      "2020/11/01 18:32:44\n",
      "epoch: 22/150 | trn loss: 1.5320 | val loss: 1.4840 | val accuracy: 45.1422% \n",
      "\n",
      "Model replaced and saved as  Custom_model_1.9_1.484\n",
      "2020/11/01 18:34:42\n",
      "epoch: 23/150 | trn loss: 1.4920 | val loss: 1.4405 | val accuracy: 46.7147% \n",
      "\n",
      "Model replaced and saved as  Custom_model_1.9_1.440\n",
      "2020/11/01 18:36:40\n",
      "epoch: 24/150 | trn loss: 1.4543 | val loss: 1.4770 | val accuracy: 45.4928% \n",
      "\n",
      "2020/11/01 18:38:36\n",
      "epoch: 25/150 | trn loss: 1.4252 | val loss: 1.3809 | val accuracy: 49.6494% \n",
      "\n",
      "Model replaced and saved as  Custom_model_1.9_1.381\n",
      "2020/11/01 18:40:33\n",
      "epoch: 26/150 | trn loss: 1.3856 | val loss: 1.3857 | val accuracy: 49.3890% \n",
      "\n",
      "2020/11/01 18:42:30\n",
      "epoch: 27/150 | trn loss: 1.3373 | val loss: 1.3201 | val accuracy: 52.4740% \n",
      "\n",
      "Model replaced and saved as  Custom_model_1.9_1.320\n",
      "2020/11/01 18:44:27\n",
      "epoch: 28/150 | trn loss: 1.3241 | val loss: 1.3724 | val accuracy: 50.8614% \n",
      "\n",
      "2020/11/01 18:46:23\n",
      "epoch: 29/150 | trn loss: 1.3181 | val loss: 1.2844 | val accuracy: 53.5657% \n",
      "\n",
      "Model replaced and saved as  Custom_model_1.9_1.284\n",
      "2020/11/01 18:48:20\n",
      "epoch: 30/150 | trn loss: 1.2837 | val loss: 1.3107 | val accuracy: 52.4740% \n",
      "\n",
      "2020/11/01 18:50:16\n",
      "epoch: 31/150 | trn loss: 1.1885 | val loss: 1.1469 | val accuracy: 58.2632% \n",
      "\n",
      "Model replaced and saved as  Custom_model_1.9_1.147\n",
      "2020/11/01 18:52:14\n",
      "epoch: 32/150 | trn loss: 1.1054 | val loss: 1.0843 | val accuracy: 60.7372% \n",
      "\n",
      "Model replaced and saved as  Custom_model_1.9_1.084\n",
      "2020/11/01 18:54:12\n",
      "epoch: 33/150 | trn loss: 1.0495 | val loss: 1.0493 | val accuracy: 62.3898% \n",
      "\n",
      "Model replaced and saved as  Custom_model_1.9_1.049\n",
      "2020/11/01 18:56:10\n",
      "epoch: 34/150 | trn loss: 1.0201 | val loss: 1.0230 | val accuracy: 63.2612% \n",
      "\n",
      "Model replaced and saved as  Custom_model_1.9_1.023\n",
      "2020/11/01 18:58:08\n",
      "epoch: 35/150 | trn loss: 1.0024 | val loss: 0.9997 | val accuracy: 63.8421% \n",
      "\n",
      "Model replaced and saved as  Custom_model_1.9_1.000\n",
      "2020/11/01 19:00:05\n",
      "epoch: 36/150 | trn loss: 0.9780 | val loss: 0.9923 | val accuracy: 64.5032% \n",
      "\n",
      "Model replaced and saved as  Custom_model_1.9_0.992\n",
      "2020/11/01 19:02:03\n",
      "epoch: 37/150 | trn loss: 0.9685 | val loss: 0.9823 | val accuracy: 64.3229% \n",
      "\n",
      "Model replaced and saved as  Custom_model_1.9_0.982\n",
      "2020/11/01 19:04:01\n",
      "epoch: 38/150 | trn loss: 0.9485 | val loss: 0.9400 | val accuracy: 66.2059% \n",
      "\n",
      "Model replaced and saved as  Custom_model_1.9_0.940\n",
      "2020/11/01 19:05:58\n",
      "epoch: 39/150 | trn loss: 0.9365 | val loss: 0.9300 | val accuracy: 66.6166% \n",
      "\n",
      "Model replaced and saved as  Custom_model_1.9_0.930\n",
      "2020/11/01 19:07:56\n",
      "epoch: 40/150 | trn loss: 0.8950 | val loss: 0.9058 | val accuracy: 67.3077% \n",
      "\n",
      "Model replaced and saved as  Custom_model_1.9_0.906\n",
      "2020/11/01 19:09:54\n",
      "epoch: 41/150 | trn loss: 0.8901 | val loss: 0.9326 | val accuracy: 66.5264% \n",
      "\n",
      "2020/11/01 19:11:50\n",
      "epoch: 42/150 | trn loss: 0.8999 | val loss: 0.9302 | val accuracy: 66.6567% \n",
      "\n",
      "2020/11/01 19:13:46\n",
      "epoch: 43/150 | trn loss: 0.8611 | val loss: 0.8789 | val accuracy: 68.2292% \n",
      "\n",
      "Model replaced and saved as  Custom_model_1.9_0.879\n",
      "2020/11/01 19:15:44\n",
      "epoch: 44/150 | trn loss: 0.8480 | val loss: 0.8626 | val accuracy: 68.9203% \n",
      "\n",
      "Model replaced and saved as  Custom_model_1.9_0.863\n",
      "2020/11/01 19:17:42\n",
      "epoch: 45/150 | trn loss: 0.8173 | val loss: 0.8431 | val accuracy: 69.9319% \n",
      "\n",
      "Model replaced and saved as  Custom_model_1.9_0.843\n",
      "2020/11/01 19:19:40\n",
      "epoch: 46/150 | trn loss: 0.8208 | val loss: 0.8472 | val accuracy: 70.2624% \n",
      "\n",
      "2020/11/01 19:21:37\n",
      "epoch: 47/150 | trn loss: 0.7951 | val loss: 0.8345 | val accuracy: 70.2123% \n",
      "\n",
      "Model replaced and saved as  Custom_model_1.9_0.834\n",
      "2020/11/01 19:23:35\n",
      "epoch: 48/150 | trn loss: 0.8910 | val loss: 0.8608 | val accuracy: 69.2107% \n",
      "\n",
      "2020/11/01 19:25:31\n",
      "epoch: 49/150 | trn loss: 0.8237 | val loss: 0.8338 | val accuracy: 69.8518% \n",
      "\n",
      "Model replaced and saved as  Custom_model_1.9_0.834\n",
      "2020/11/01 19:27:29\n",
      "epoch: 50/150 | trn loss: 0.7914 | val loss: 0.8025 | val accuracy: 71.4744% \n",
      "\n",
      "Model replaced and saved as  Custom_model_1.9_0.803\n",
      "2020/11/01 19:29:27\n",
      "epoch: 51/150 | trn loss: 0.7554 | val loss: 0.7789 | val accuracy: 72.6262% \n",
      "\n",
      "Model replaced and saved as  Custom_model_1.9_0.779\n",
      "2020/11/01 19:31:24\n",
      "epoch: 52/150 | trn loss: 0.7549 | val loss: 0.7878 | val accuracy: 71.8650% \n",
      "\n",
      "2020/11/01 19:33:20\n",
      "epoch: 53/150 | trn loss: 0.7283 | val loss: 0.7499 | val accuracy: 73.5076% \n",
      "\n",
      "Model replaced and saved as  Custom_model_1.9_0.750\n",
      "2020/11/01 19:35:18\n",
      "epoch: 54/150 | trn loss: 0.7177 | val loss: 0.7580 | val accuracy: 73.2572% \n",
      "\n",
      "2020/11/01 19:37:14\n",
      "epoch: 55/150 | trn loss: 0.7074 | val loss: 0.7177 | val accuracy: 74.5693% \n",
      "\n",
      "Model replaced and saved as  Custom_model_1.9_0.718\n",
      "2020/11/01 19:39:12\n",
      "epoch: 56/150 | trn loss: 0.7101 | val loss: 0.7200 | val accuracy: 74.4992% \n",
      "\n",
      "2020/11/01 19:41:08\n",
      "epoch: 57/150 | trn loss: 0.6774 | val loss: 0.7093 | val accuracy: 74.7196% \n",
      "\n",
      "Model replaced and saved as  Custom_model_1.9_0.709\n",
      "2020/11/01 19:43:06\n",
      "epoch: 58/150 | trn loss: 0.6674 | val loss: 0.6784 | val accuracy: 76.3421% \n",
      "\n",
      "Model replaced and saved as  Custom_model_1.9_0.678\n",
      "2020/11/01 19:45:04\n",
      "epoch: 59/150 | trn loss: 0.6791 | val loss: 0.6875 | val accuracy: 75.7612% \n",
      "\n",
      "2020/11/01 19:47:00\n",
      "epoch: 60/150 | trn loss: 0.6485 | val loss: 0.6583 | val accuracy: 77.2236% \n",
      "\n",
      "Model replaced and saved as  Custom_model_1.9_0.658\n",
      "2020/11/01 19:48:58\n",
      "epoch: 61/150 | trn loss: 0.5788 | val loss: 0.5881 | val accuracy: 79.8678% \n",
      "\n",
      "Model replaced and saved as  Custom_model_1.9_0.588\n",
      "2020/11/01 19:50:56\n",
      "epoch: 62/150 | trn loss: 0.5474 | val loss: 0.5729 | val accuracy: 80.4387% \n",
      "\n",
      "Model replaced and saved as  Custom_model_1.9_0.573\n",
      "2020/11/01 19:52:53\n",
      "epoch: 63/150 | trn loss: 0.5241 | val loss: 0.5735 | val accuracy: 80.1883% \n",
      "\n",
      "2020/11/01 19:54:50\n",
      "epoch: 64/150 | trn loss: 0.5152 | val loss: 0.5276 | val accuracy: 81.7308% \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model replaced and saved as  Custom_model_1.9_0.528\n",
      "2020/11/01 19:56:47\n",
      "epoch: 65/150 | trn loss: 0.5161 | val loss: 0.5443 | val accuracy: 81.2099% \n",
      "\n",
      "2020/11/01 19:58:43\n",
      "epoch: 66/150 | trn loss: 0.4980 | val loss: 0.5354 | val accuracy: 81.4403% \n",
      "\n",
      "2020/11/01 20:00:39\n",
      "epoch: 67/150 | trn loss: 0.5027 | val loss: 0.5309 | val accuracy: 81.7308% \n",
      "\n",
      "2020/11/01 20:02:35\n",
      "epoch: 68/150 | trn loss: 0.4982 | val loss: 0.5327 | val accuracy: 82.0813% \n",
      "\n",
      "2020/11/01 20:04:31\n",
      "epoch: 69/150 | trn loss: 0.4907 | val loss: 0.5170 | val accuracy: 82.1114% \n",
      "\n",
      "Model replaced and saved as  Custom_model_1.9_0.517\n",
      "2020/11/01 20:06:29\n",
      "epoch: 70/150 | trn loss: 0.5335 | val loss: 0.5452 | val accuracy: 80.9796% \n",
      "\n",
      "2020/11/01 20:08:25\n",
      "epoch: 71/150 | trn loss: 0.5058 | val loss: 0.5250 | val accuracy: 81.2300% \n",
      "\n",
      "2020/11/01 20:10:22\n",
      "epoch: 72/150 | trn loss: 0.6166 | val loss: 0.7686 | val accuracy: 73.3774% \n",
      "\n",
      "2020/11/01 20:12:19\n",
      "epoch: 73/150 | trn loss: 0.6253 | val loss: 0.6100 | val accuracy: 78.8962% \n",
      "\n",
      "2020/11/01 20:14:15\n",
      "epoch: 74/150 | trn loss: 0.5546 | val loss: 0.5736 | val accuracy: 80.1082% \n",
      "\n",
      "2020/11/01 20:16:11\n",
      "epoch: 75/150 | trn loss: 0.5150 | val loss: 0.5604 | val accuracy: 80.4287% \n",
      "\n",
      "2020/11/01 20:18:07\n",
      "epoch: 76/150 | trn loss: 0.4972 | val loss: 0.5335 | val accuracy: 81.5505% \n",
      "\n",
      "2020/11/01 20:20:03\n",
      "epoch: 77/150 | trn loss: 0.4987 | val loss: 0.5256 | val accuracy: 81.8610% \n",
      "\n",
      "2020/11/01 20:21:59\n",
      "epoch: 78/150 | trn loss: 0.4882 | val loss: 0.5212 | val accuracy: 82.5421% \n",
      "\n",
      "2020/11/01 20:23:55\n",
      "epoch: 79/150 | trn loss: 0.4815 | val loss: 0.5009 | val accuracy: 82.7324% \n",
      "\n",
      "Model replaced and saved as  Custom_model_1.9_0.501\n",
      "2020/11/01 20:25:54\n",
      "epoch: 80/150 | trn loss: 0.4669 | val loss: 0.4886 | val accuracy: 83.0729% \n",
      "\n",
      "Model replaced and saved as  Custom_model_1.9_0.489\n",
      "2020/11/01 20:27:51\n",
      "epoch: 81/150 | trn loss: 0.4698 | val loss: 0.4714 | val accuracy: 84.0345% \n",
      "\n",
      "Model replaced and saved as  Custom_model_1.9_0.471\n",
      "2020/11/01 20:29:49\n",
      "epoch: 82/150 | trn loss: 0.4412 | val loss: 0.4819 | val accuracy: 83.1831% \n",
      "\n",
      "2020/11/01 20:31:45\n",
      "epoch: 83/150 | trn loss: 0.4821 | val loss: 0.4785 | val accuracy: 83.3834% \n",
      "\n",
      "2020/11/01 20:33:41\n",
      "epoch: 84/150 | trn loss: 0.4383 | val loss: 0.4668 | val accuracy: 84.1346% \n",
      "\n",
      "Model replaced and saved as  Custom_model_1.9_0.467\n",
      "2020/11/01 20:35:39\n",
      "epoch: 85/150 | trn loss: 0.4374 | val loss: 0.4461 | val accuracy: 85.0561% \n",
      "\n",
      "Model replaced and saved as  Custom_model_1.9_0.446\n",
      "2020/11/01 20:37:36\n",
      "epoch: 86/150 | trn loss: 0.4236 | val loss: 0.4687 | val accuracy: 84.1546% \n",
      "\n",
      "2020/11/01 20:39:33\n",
      "epoch: 87/150 | trn loss: 0.4204 | val loss: 0.4612 | val accuracy: 84.4351% \n",
      "\n",
      "2020/11/01 20:41:29\n",
      "epoch: 88/150 | trn loss: 0.4136 | val loss: 0.4546 | val accuracy: 84.5653% \n",
      "\n",
      "2020/11/01 20:43:25\n",
      "epoch: 89/150 | trn loss: 0.4119 | val loss: 0.4373 | val accuracy: 85.1963% \n",
      "\n",
      "Model replaced and saved as  Custom_model_1.9_0.437\n",
      "2020/11/01 20:45:23\n",
      "epoch: 90/150 | trn loss: 0.4087 | val loss: 0.4366 | val accuracy: 85.1863% \n",
      "\n",
      "Model replaced and saved as  Custom_model_1.9_0.437\n",
      "2020/11/01 20:47:20\n",
      "epoch: 91/150 | trn loss: 0.3907 | val loss: 0.4184 | val accuracy: 85.7472% \n",
      "\n",
      "Model replaced and saved as  Custom_model_1.9_0.418\n",
      "2020/11/01 20:49:18\n",
      "epoch: 92/150 | trn loss: 0.3868 | val loss: 0.4067 | val accuracy: 86.1278% \n",
      "\n",
      "Model replaced and saved as  Custom_model_1.9_0.407\n",
      "2020/11/01 20:51:15\n",
      "epoch: 93/150 | trn loss: 0.3887 | val loss: 0.4176 | val accuracy: 86.0076% \n",
      "\n",
      "2020/11/01 20:53:11\n",
      "epoch: 94/150 | trn loss: 0.3788 | val loss: 0.3984 | val accuracy: 86.6386% \n",
      "\n",
      "Model replaced and saved as  Custom_model_1.9_0.398\n",
      "2020/11/01 20:55:09\n",
      "epoch: 95/150 | trn loss: 0.3852 | val loss: 0.4052 | val accuracy: 86.2280% \n",
      "\n",
      "2020/11/01 20:57:05\n",
      "epoch: 96/150 | trn loss: 0.3783 | val loss: 0.3936 | val accuracy: 86.8890% \n",
      "\n",
      "Model replaced and saved as  Custom_model_1.9_0.394\n",
      "2020/11/01 20:59:03\n",
      "epoch: 97/150 | trn loss: 0.3741 | val loss: 0.4116 | val accuracy: 86.1779% \n",
      "\n",
      "2020/11/01 21:00:59\n",
      "epoch: 98/150 | trn loss: 0.3707 | val loss: 0.4087 | val accuracy: 86.2380% \n",
      "\n",
      "2020/11/01 21:02:56\n",
      "epoch: 99/150 | trn loss: 0.3794 | val loss: 0.3939 | val accuracy: 86.8490% \n",
      "\n",
      "2020/11/01 21:04:52\n",
      "epoch: 100/150 | trn loss: 0.3743 | val loss: 0.3989 | val accuracy: 86.8189% \n",
      "\n",
      "2020/11/01 21:06:48\n",
      "epoch: 101/150 | trn loss: 0.3783 | val loss: 0.4094 | val accuracy: 86.1779% \n",
      "\n",
      "2020/11/01 21:08:44\n",
      "epoch: 102/150 | trn loss: 0.3799 | val loss: 0.3968 | val accuracy: 86.6386% \n",
      "\n",
      "2020/11/01 21:10:41\n",
      "epoch: 103/150 | trn loss: 0.3794 | val loss: 0.3937 | val accuracy: 86.9391% \n",
      "\n",
      "2020/11/01 21:12:37\n",
      "epoch: 104/150 | trn loss: 0.3714 | val loss: 0.4030 | val accuracy: 86.3682% \n",
      "\n",
      "2020/11/01 21:14:33\n",
      "epoch: 105/150 | trn loss: 0.3616 | val loss: 0.3836 | val accuracy: 87.2095% \n",
      "\n",
      "Model replaced and saved as  Custom_model_1.9_0.384\n",
      "2020/11/01 21:16:30\n",
      "epoch: 106/150 | trn loss: 0.3730 | val loss: 0.3863 | val accuracy: 87.0092% \n",
      "\n",
      "2020/11/01 21:18:26\n",
      "epoch: 107/150 | trn loss: 0.3600 | val loss: 0.3901 | val accuracy: 86.8890% \n",
      "\n",
      "2020/11/01 21:20:22\n",
      "epoch: 108/150 | trn loss: 0.3645 | val loss: 0.4039 | val accuracy: 86.2380% \n",
      "\n",
      "2020/11/01 21:22:18\n",
      "epoch: 109/150 | trn loss: 0.3690 | val loss: 0.3753 | val accuracy: 87.2897% \n",
      "\n",
      "Model replaced and saved as  Custom_model_1.9_0.375\n",
      "2020/11/01 21:24:16\n",
      "epoch: 110/150 | trn loss: 0.3675 | val loss: 0.3994 | val accuracy: 86.1378% \n",
      "\n",
      "2020/11/01 21:26:12\n",
      "epoch: 111/150 | trn loss: 0.3559 | val loss: 0.3857 | val accuracy: 87.1194% \n",
      "\n",
      "2020/11/01 21:28:08\n",
      "epoch: 112/150 | trn loss: 0.3603 | val loss: 0.3990 | val accuracy: 86.4984% \n",
      "\n",
      "2020/11/01 21:30:04\n",
      "epoch: 113/150 | trn loss: 0.3567 | val loss: 0.3986 | val accuracy: 86.6987% \n",
      "\n",
      "2020/11/01 21:32:00\n",
      "epoch: 114/150 | trn loss: 0.3539 | val loss: 0.3809 | val accuracy: 87.0793% \n",
      "\n",
      "2020/11/01 21:33:57\n",
      "epoch: 115/150 | trn loss: 0.3601 | val loss: 0.3730 | val accuracy: 87.0092% \n",
      "\n",
      "Model replaced and saved as  Custom_model_1.9_0.373\n",
      "2020/11/01 21:35:54\n",
      "epoch: 116/150 | trn loss: 0.3651 | val loss: 0.3927 | val accuracy: 86.2780% \n",
      "\n",
      "2020/11/01 21:37:50\n",
      "epoch: 117/150 | trn loss: 0.3667 | val loss: 0.3691 | val accuracy: 87.4399% \n",
      "\n",
      "Model replaced and saved as  Custom_model_1.9_0.369\n",
      "2020/11/01 21:39:48\n",
      "epoch: 118/150 | trn loss: 0.3464 | val loss: 0.3732 | val accuracy: 87.6502% \n",
      "\n",
      "2020/11/01 21:41:44\n",
      "epoch: 119/150 | trn loss: 0.3591 | val loss: 0.3710 | val accuracy: 87.7504% \n",
      "\n",
      "2020/11/01 21:43:40\n",
      "epoch: 120/150 | trn loss: 0.3614 | val loss: 0.3742 | val accuracy: 87.5901% \n",
      "\n",
      "2020/11/01 21:45:36\n",
      "epoch: 121/150 | trn loss: 0.3604 | val loss: 0.3886 | val accuracy: 86.7688% \n",
      "\n",
      "2020/11/01 21:47:32\n",
      "epoch: 122/150 | trn loss: 0.3527 | val loss: 0.3762 | val accuracy: 87.1695% \n",
      "\n",
      "2020/11/01 21:49:28\n",
      "epoch: 123/150 | trn loss: 0.3576 | val loss: 0.3719 | val accuracy: 87.6402% \n",
      "\n",
      "2020/11/01 21:51:24\n",
      "epoch: 124/150 | trn loss: 0.3523 | val loss: 0.3715 | val accuracy: 87.4099% \n",
      "\n",
      "2020/11/01 21:53:20\n",
      "epoch: 125/150 | trn loss: 0.3630 | val loss: 0.3726 | val accuracy: 87.7003% \n",
      "\n",
      "2020/11/01 21:55:17\n",
      "epoch: 126/150 | trn loss: 0.3481 | val loss: 0.3870 | val accuracy: 86.8490% \n",
      "\n",
      "2020/11/01 21:57:13\n",
      "epoch: 127/150 | trn loss: 0.3540 | val loss: 0.3752 | val accuracy: 87.5000% \n",
      "\n",
      "2020/11/01 21:59:09\n",
      "epoch: 128/150 | trn loss: 0.3564 | val loss: 0.3718 | val accuracy: 87.3598% \n",
      "\n",
      "2020/11/01 22:01:05\n",
      "epoch: 129/150 | trn loss: 0.3495 | val loss: 0.3783 | val accuracy: 87.3898% \n",
      "\n",
      "2020/11/01 22:03:01\n",
      "epoch: 130/150 | trn loss: 0.3501 | val loss: 0.3791 | val accuracy: 87.1394% \n",
      "\n",
      "2020/11/01 22:04:57\n",
      "epoch: 131/150 | trn loss: 0.3506 | val loss: 0.3804 | val accuracy: 87.4599% \n",
      "\n",
      "2020/11/01 22:06:53\n",
      "epoch: 132/150 | trn loss: 0.3640 | val loss: 0.3699 | val accuracy: 87.5901% \n",
      "\n",
      "2020/11/01 22:08:49\n",
      "epoch: 133/150 | trn loss: 0.3559 | val loss: 0.3794 | val accuracy: 87.4700% \n",
      "\n",
      "2020/11/01 22:10:45\n",
      "epoch: 134/150 | trn loss: 0.3592 | val loss: 0.3821 | val accuracy: 87.1194% \n",
      "\n",
      "2020/11/01 22:12:41\n",
      "epoch: 135/150 | trn loss: 0.3552 | val loss: 0.3716 | val accuracy: 87.3698% \n",
      "\n",
      "2020/11/01 22:14:37\n",
      "epoch: 136/150 | trn loss: 0.3518 | val loss: 0.3671 | val accuracy: 87.7704% \n",
      "\n",
      "Model replaced and saved as  Custom_model_1.9_0.367\n",
      "2020/11/01 22:16:35\n",
      "epoch: 137/150 | trn loss: 0.3610 | val loss: 0.3760 | val accuracy: 87.5601% \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/11/01 22:18:31\n",
      "epoch: 138/150 | trn loss: 0.3498 | val loss: 0.3707 | val accuracy: 87.3798% \n",
      "\n",
      "2020/11/01 22:20:27\n",
      "epoch: 139/150 | trn loss: 0.3469 | val loss: 0.3509 | val accuracy: 88.0609% \n",
      "\n",
      "Model replaced and saved as  Custom_model_1.9_0.351\n",
      "2020/11/01 22:22:25\n",
      "epoch: 140/150 | trn loss: 0.3538 | val loss: 0.3763 | val accuracy: 87.3698% \n",
      "\n",
      "2020/11/01 22:24:21\n",
      "epoch: 141/150 | trn loss: 0.3520 | val loss: 0.3856 | val accuracy: 87.1194% \n",
      "\n",
      "2020/11/01 22:26:17\n",
      "epoch: 142/150 | trn loss: 0.3485 | val loss: 0.3811 | val accuracy: 87.0393% \n",
      "\n",
      "2020/11/01 22:28:13\n",
      "epoch: 143/150 | trn loss: 0.3458 | val loss: 0.3760 | val accuracy: 87.1394% \n",
      "\n",
      "2020/11/01 22:30:09\n",
      "epoch: 144/150 | trn loss: 0.3514 | val loss: 0.3650 | val accuracy: 87.8906% \n",
      "\n",
      "2020/11/01 22:32:05\n",
      "epoch: 145/150 | trn loss: 0.3525 | val loss: 0.3756 | val accuracy: 87.1895% \n",
      "\n",
      "2020/11/01 22:34:01\n",
      "epoch: 146/150 | trn loss: 0.3492 | val loss: 0.3782 | val accuracy: 87.1795% \n",
      "\n",
      "2020/11/01 22:35:57\n",
      "epoch: 147/150 | trn loss: 0.3542 | val loss: 0.3744 | val accuracy: 87.2095% \n",
      "\n",
      "2020/11/01 22:37:53\n",
      "epoch: 148/150 | trn loss: 0.3555 | val loss: 0.3838 | val accuracy: 86.9191% \n",
      "\n",
      "2020/11/01 22:39:50\n",
      "epoch: 149/150 | trn loss: 0.3543 | val loss: 0.3649 | val accuracy: 87.7304% \n",
      "\n",
      "2020/11/01 22:41:46\n",
      "epoch: 150/150 | trn loss: 0.3595 | val loss: 0.3769 | val accuracy: 87.6102% \n",
      "\n"
     ]
    }
   ],
   "source": [
    "trn_loss_list = []\n",
    "val_loss_list = []\n",
    "val_acc_list = []\n",
    "total_epoch=150\n",
    "model_char=\"1.9\"\n",
    "model_name=\"\"\n",
    "patience=10\n",
    "start_early_stop_check=0\n",
    "saving_start_epoch=10\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    trn_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        if torch.cuda.is_available():\n",
    "            inputs=inputs.cuda()\n",
    "            labels=labels.cuda()\n",
    "        # grad init\n",
    "        optimizer.zero_grad()\n",
    "        # forward propagation\n",
    "        output= model(inputs)\n",
    "        # calculate loss\n",
    "        loss=criterion(output, labels)\n",
    "        # back propagation \n",
    "        loss.backward()\n",
    "        # weight update\n",
    "        optimizer.step()\n",
    "        \n",
    "        # trn_loss summary\n",
    "        trn_loss += loss.item()\n",
    "        # del (memory issue)\n",
    "        del loss\n",
    "        del output\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        cor_match = 0\n",
    "        for j, val in enumerate(test_loader):\n",
    "            val_x, val_label = val\n",
    "            if torch.cuda.is_available():\n",
    "                val_x = val_x.cuda()\n",
    "                val_label =val_label.cuda()\n",
    "            val_output = model(val_x)\n",
    "            v_loss = criterion(val_output, val_label)\n",
    "            val_loss += v_loss\n",
    "            _, predicted=torch.max(val_output,1)\n",
    "            cor_match+=np.count_nonzero(predicted.cpu().detach()==val_label.cpu().detach())\n",
    "    del val_output\n",
    "    del v_loss\n",
    "    del predicted\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    \n",
    "    \n",
    "    trn_loss_list.append(trn_loss/len(train_loader))\n",
    "    val_loss_list.append(val_loss/len(test_loader))\n",
    "    val_acc=cor_match/(len(test_loader)*batch_size)\n",
    "    val_acc_list.append(val_acc)\n",
    "    now = time.localtime()\n",
    "    print (\"%04d/%02d/%02d %02d:%02d:%02d\" % (now.tm_year, now.tm_mon, now.tm_mday, now.tm_hour, now.tm_min, now.tm_sec))\n",
    "\n",
    "    print(\"epoch: {}/{} | trn loss: {:.4f} | val loss: {:.4f} | val accuracy: {:.4f}% \\n\".format(\n",
    "                epoch+1, total_epoch, trn_loss / len(train_loader), val_loss / len(test_loader), val_acc*100\n",
    "            ))\n",
    "    \n",
    "    \n",
    "    if epoch+1>2:\n",
    "        if val_loss_list[-1]>val_loss_list[-2]:\n",
    "            start_early_stop_check=1\n",
    "    else:\n",
    "        val_loss_min=val_loss_list[-1]\n",
    "        \n",
    "    if start_early_stop_check:\n",
    "        early_stop_temp=val_loss_list[-patience:]\n",
    "        if all(early_stop_temp[i]<early_stop_temp[i+1] for i in range (len(early_stop_temp)-1)):\n",
    "            print(\"Early stop!\")\n",
    "            break\n",
    "            \n",
    "    if epoch+1>saving_start_epoch:\n",
    "        if val_loss_list[-1]<val_loss_min:\n",
    "            if os.path.isfile(model_name):\n",
    "                os.remove(model_name)\n",
    "            val_loss_min=val_loss_list[-1]\n",
    "            model_name=\"Custom_model_\"+model_char+\"_{:.3f}\".format(val_loss_min)\n",
    "            torch.save(model, model_name)\n",
    "            print(\"Model replaced and saved as \",model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plt.ylabel(\"val_accuracy\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(val_acc_list)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_acc_list=np.array(val_acc_list)\n",
    "np.savetxt(\"ver_1.9.txt\", val_acc_list, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import time\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (6): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (7): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (8): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (9): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (10): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (11): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (12): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (13): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (14): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (15): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (16): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (17): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (18): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (19): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (20): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (21): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (22): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 4096, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(4096, 4096, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(4096, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 4096, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(4096, 4096, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(4096, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 4096, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(4096, 4096, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(4096, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=2048, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-deeplearning/Organize\n"
     ]
    }
   ],
   "source": [
    "%cd /home/jupyter-deeplearning/Organize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = torchvision.datasets.ImageFolder('./data/test', transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "Category = []\n",
    "for input, _ in test_loader:\n",
    "    input = input.cuda()\n",
    "    output = model(input)\n",
    "    output = torch.argmax(output, dim=1)\n",
    "    Category = Category + output.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!!\n"
     ]
    }
   ],
   "source": [
    "Id = list(range(0, 8000))\n",
    "samples = {\n",
    "   'Id': Id,\n",
    "   'Category': Category \n",
    "}\n",
    "df = pd.DataFrame(samples, columns=['Id', 'Category'])\n",
    "\n",
    "df.to_csv('submission.csv', index=False)\n",
    "print('Done!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
