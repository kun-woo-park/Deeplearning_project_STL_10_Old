{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import torch.distributed as dist\n",
    "import math\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import Type, Any, Callable, Union, List, Optional\n",
    "from torch import Tensor\n",
    "from torch.hub import load_state_dict_from_url\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.data.sampler import BatchSampler\n",
    "from collections.abc import Mapping, Sequence\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path_path= f\"./data/split_train/train\"\n",
    "test_path_path= f\"./data/split_train/val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "num_gpus=4\n",
    "num_workers=64\n",
    "lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.507, 0.487, 0.441],\n",
    "                                     std=[0.267, 0.256, 0.276])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder(\n",
    "        train_path_path,\n",
    "        transforms.Compose([\n",
    "        transforms.RandomRotation(degrees=45),\n",
    "            \n",
    "        transforms.RandomResizedCrop(70),\n",
    "        transforms.ColorJitter(.3,.3,.3,.3),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "for _ in range (19):\n",
    "    aug_data = datasets.ImageFolder(\n",
    "        train_path_path,\n",
    "        transforms.Compose([\n",
    "        transforms.RandomRotation(degrees=45),\n",
    "            \n",
    "        transforms.RandomResizedCrop(70),\n",
    "        transforms.ColorJitter(.3,.3,.3,.3),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "    dataset=torch.utils.data.ConcatDataset([dataset,aug_data])\n",
    "valset = datasets.ImageFolder(\n",
    "        test_path_path,\n",
    "        transforms.Compose([\n",
    "        transforms.RandomRotation(degrees=45),\n",
    "            \n",
    "        transforms.RandomResizedCrop(70),\n",
    "        transforms.ColorJitter(.3,.3,.3,.3),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "for _ in range (19):\n",
    "    aug_data = datasets.ImageFolder(\n",
    "        test_path_path,\n",
    "        transforms.Compose([\n",
    "        transforms.RandomRotation(degrees=45),\n",
    "            \n",
    "        transforms.RandomResizedCrop(70),\n",
    "        transforms.ColorJitter(.3,.3,.3,.3),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "    valset=torch.utils.data.ConcatDataset([valset,aug_data])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=True,\n",
    "        num_workers=8, pin_memory=True,drop_last=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        valset, batch_size=batch_size, shuffle=True,\n",
    "        num_workers=8, pin_memory=True,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
    "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
    "    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n",
    "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
    "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
    "\n",
    "    expansion: int = 4\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: Optional[nn.Module] = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
    "    ) -> None:\n",
    "        super(Bottleneck, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        width = int(planes * (base_width / 64.)) * groups\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.LeakyReLU(0.1)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        block: Type[Union[Bottleneck]],\n",
    "        layers: List[int],\n",
    "        num_classes: int = 10,\n",
    "        zero_init_residual: bool = False,\n",
    "        groups: int = 1,\n",
    "        width_per_group: int = 64,\n",
    "        replace_stride_with_dilation: Optional[List[bool]] = None,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
    "    ) -> None:\n",
    "        super(ResNet, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
    "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=4, stride=1,\n",
    "                               bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.LeakyReLU(0.1)\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.layer1 = self._make_layer(block, 30, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 60, layers[1], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 96, layers[2], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[1])\n",
    "        \n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(96 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]\n",
    "\n",
    "    def _make_layer(self, block: Type[Union[Bottleneck]], planes: int, blocks: int,\n",
    "                    stride: int = 1, dilate: bool = False) -> nn.Sequential:\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "                            self.base_width, previous_dilation, norm_layer))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation,\n",
    "                                norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x,1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self._forward_impl(x)\n",
    "\n",
    "\n",
    "def _resnet(\n",
    "    arch: str,\n",
    "    block: Type[Union[Bottleneck]],\n",
    "    layers: List[int],\n",
    "    pretrained: bool,\n",
    "    progress: bool,\n",
    "    **kwargs: Any\n",
    ") -> ResNet:\n",
    "    model = ResNet(block, layers, **kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
    "                                              progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "def _resnext(arch, block, layers, pretrained, progress, **kwargs):\n",
    "    model = ResNet(block, layers, **kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = load_state_dict_from_url(model_urls[arch], progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model(pretrained: bool = False, progress: bool = True, **kwargs):\n",
    "    \n",
    "    kwargs['groups'] = 1\n",
    "    kwargs['width_per_group'] = 64\n",
    "    return _resnext('resnext', Bottleneck, [4, 9, 8], pretrained, progress, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.1, dim=-1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print (name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 93, 93]           3,072\n",
      "       BatchNorm2d-2           [-1, 64, 93, 93]             128\n",
      "         LeakyReLU-3           [-1, 64, 93, 93]               0\n",
      "         MaxPool2d-4           [-1, 64, 46, 46]               0\n",
      "            Conv2d-5           [-1, 30, 46, 46]           1,920\n",
      "       BatchNorm2d-6           [-1, 30, 46, 46]              60\n",
      "         LeakyReLU-7           [-1, 30, 46, 46]               0\n",
      "            Conv2d-8           [-1, 30, 46, 46]           8,100\n",
      "       BatchNorm2d-9           [-1, 30, 46, 46]              60\n",
      "        LeakyReLU-10           [-1, 30, 46, 46]               0\n",
      "           Conv2d-11          [-1, 120, 46, 46]           3,600\n",
      "      BatchNorm2d-12          [-1, 120, 46, 46]             240\n",
      "           Conv2d-13          [-1, 120, 46, 46]           7,680\n",
      "      BatchNorm2d-14          [-1, 120, 46, 46]             240\n",
      "        LeakyReLU-15          [-1, 120, 46, 46]               0\n",
      "       Bottleneck-16          [-1, 120, 46, 46]               0\n",
      "           Conv2d-17           [-1, 30, 46, 46]           3,600\n",
      "      BatchNorm2d-18           [-1, 30, 46, 46]              60\n",
      "        LeakyReLU-19           [-1, 30, 46, 46]               0\n",
      "           Conv2d-20           [-1, 30, 46, 46]           8,100\n",
      "      BatchNorm2d-21           [-1, 30, 46, 46]              60\n",
      "        LeakyReLU-22           [-1, 30, 46, 46]               0\n",
      "           Conv2d-23          [-1, 120, 46, 46]           3,600\n",
      "      BatchNorm2d-24          [-1, 120, 46, 46]             240\n",
      "        LeakyReLU-25          [-1, 120, 46, 46]               0\n",
      "       Bottleneck-26          [-1, 120, 46, 46]               0\n",
      "           Conv2d-27           [-1, 30, 46, 46]           3,600\n",
      "      BatchNorm2d-28           [-1, 30, 46, 46]              60\n",
      "        LeakyReLU-29           [-1, 30, 46, 46]               0\n",
      "           Conv2d-30           [-1, 30, 46, 46]           8,100\n",
      "      BatchNorm2d-31           [-1, 30, 46, 46]              60\n",
      "        LeakyReLU-32           [-1, 30, 46, 46]               0\n",
      "           Conv2d-33          [-1, 120, 46, 46]           3,600\n",
      "      BatchNorm2d-34          [-1, 120, 46, 46]             240\n",
      "        LeakyReLU-35          [-1, 120, 46, 46]               0\n",
      "       Bottleneck-36          [-1, 120, 46, 46]               0\n",
      "           Conv2d-37           [-1, 30, 46, 46]           3,600\n",
      "      BatchNorm2d-38           [-1, 30, 46, 46]              60\n",
      "        LeakyReLU-39           [-1, 30, 46, 46]               0\n",
      "           Conv2d-40           [-1, 30, 46, 46]           8,100\n",
      "      BatchNorm2d-41           [-1, 30, 46, 46]              60\n",
      "        LeakyReLU-42           [-1, 30, 46, 46]               0\n",
      "           Conv2d-43          [-1, 120, 46, 46]           3,600\n",
      "      BatchNorm2d-44          [-1, 120, 46, 46]             240\n",
      "        LeakyReLU-45          [-1, 120, 46, 46]               0\n",
      "       Bottleneck-46          [-1, 120, 46, 46]               0\n",
      "           Conv2d-47           [-1, 60, 46, 46]           7,200\n",
      "      BatchNorm2d-48           [-1, 60, 46, 46]             120\n",
      "        LeakyReLU-49           [-1, 60, 46, 46]               0\n",
      "           Conv2d-50           [-1, 60, 23, 23]          32,400\n",
      "      BatchNorm2d-51           [-1, 60, 23, 23]             120\n",
      "        LeakyReLU-52           [-1, 60, 23, 23]               0\n",
      "           Conv2d-53          [-1, 240, 23, 23]          14,400\n",
      "      BatchNorm2d-54          [-1, 240, 23, 23]             480\n",
      "           Conv2d-55          [-1, 240, 23, 23]          28,800\n",
      "      BatchNorm2d-56          [-1, 240, 23, 23]             480\n",
      "        LeakyReLU-57          [-1, 240, 23, 23]               0\n",
      "       Bottleneck-58          [-1, 240, 23, 23]               0\n",
      "           Conv2d-59           [-1, 60, 23, 23]          14,400\n",
      "      BatchNorm2d-60           [-1, 60, 23, 23]             120\n",
      "        LeakyReLU-61           [-1, 60, 23, 23]               0\n",
      "           Conv2d-62           [-1, 60, 23, 23]          32,400\n",
      "      BatchNorm2d-63           [-1, 60, 23, 23]             120\n",
      "        LeakyReLU-64           [-1, 60, 23, 23]               0\n",
      "           Conv2d-65          [-1, 240, 23, 23]          14,400\n",
      "      BatchNorm2d-66          [-1, 240, 23, 23]             480\n",
      "        LeakyReLU-67          [-1, 240, 23, 23]               0\n",
      "       Bottleneck-68          [-1, 240, 23, 23]               0\n",
      "           Conv2d-69           [-1, 60, 23, 23]          14,400\n",
      "      BatchNorm2d-70           [-1, 60, 23, 23]             120\n",
      "        LeakyReLU-71           [-1, 60, 23, 23]               0\n",
      "           Conv2d-72           [-1, 60, 23, 23]          32,400\n",
      "      BatchNorm2d-73           [-1, 60, 23, 23]             120\n",
      "        LeakyReLU-74           [-1, 60, 23, 23]               0\n",
      "           Conv2d-75          [-1, 240, 23, 23]          14,400\n",
      "      BatchNorm2d-76          [-1, 240, 23, 23]             480\n",
      "        LeakyReLU-77          [-1, 240, 23, 23]               0\n",
      "       Bottleneck-78          [-1, 240, 23, 23]               0\n",
      "           Conv2d-79           [-1, 60, 23, 23]          14,400\n",
      "      BatchNorm2d-80           [-1, 60, 23, 23]             120\n",
      "        LeakyReLU-81           [-1, 60, 23, 23]               0\n",
      "           Conv2d-82           [-1, 60, 23, 23]          32,400\n",
      "      BatchNorm2d-83           [-1, 60, 23, 23]             120\n",
      "        LeakyReLU-84           [-1, 60, 23, 23]               0\n",
      "           Conv2d-85          [-1, 240, 23, 23]          14,400\n",
      "      BatchNorm2d-86          [-1, 240, 23, 23]             480\n",
      "        LeakyReLU-87          [-1, 240, 23, 23]               0\n",
      "       Bottleneck-88          [-1, 240, 23, 23]               0\n",
      "           Conv2d-89           [-1, 60, 23, 23]          14,400\n",
      "      BatchNorm2d-90           [-1, 60, 23, 23]             120\n",
      "        LeakyReLU-91           [-1, 60, 23, 23]               0\n",
      "           Conv2d-92           [-1, 60, 23, 23]          32,400\n",
      "      BatchNorm2d-93           [-1, 60, 23, 23]             120\n",
      "        LeakyReLU-94           [-1, 60, 23, 23]               0\n",
      "           Conv2d-95          [-1, 240, 23, 23]          14,400\n",
      "      BatchNorm2d-96          [-1, 240, 23, 23]             480\n",
      "        LeakyReLU-97          [-1, 240, 23, 23]               0\n",
      "       Bottleneck-98          [-1, 240, 23, 23]               0\n",
      "           Conv2d-99           [-1, 60, 23, 23]          14,400\n",
      "     BatchNorm2d-100           [-1, 60, 23, 23]             120\n",
      "       LeakyReLU-101           [-1, 60, 23, 23]               0\n",
      "          Conv2d-102           [-1, 60, 23, 23]          32,400\n",
      "     BatchNorm2d-103           [-1, 60, 23, 23]             120\n",
      "       LeakyReLU-104           [-1, 60, 23, 23]               0\n",
      "          Conv2d-105          [-1, 240, 23, 23]          14,400\n",
      "     BatchNorm2d-106          [-1, 240, 23, 23]             480\n",
      "       LeakyReLU-107          [-1, 240, 23, 23]               0\n",
      "      Bottleneck-108          [-1, 240, 23, 23]               0\n",
      "          Conv2d-109           [-1, 60, 23, 23]          14,400\n",
      "     BatchNorm2d-110           [-1, 60, 23, 23]             120\n",
      "       LeakyReLU-111           [-1, 60, 23, 23]               0\n",
      "          Conv2d-112           [-1, 60, 23, 23]          32,400\n",
      "     BatchNorm2d-113           [-1, 60, 23, 23]             120\n",
      "       LeakyReLU-114           [-1, 60, 23, 23]               0\n",
      "          Conv2d-115          [-1, 240, 23, 23]          14,400\n",
      "     BatchNorm2d-116          [-1, 240, 23, 23]             480\n",
      "       LeakyReLU-117          [-1, 240, 23, 23]               0\n",
      "      Bottleneck-118          [-1, 240, 23, 23]               0\n",
      "          Conv2d-119           [-1, 60, 23, 23]          14,400\n",
      "     BatchNorm2d-120           [-1, 60, 23, 23]             120\n",
      "       LeakyReLU-121           [-1, 60, 23, 23]               0\n",
      "          Conv2d-122           [-1, 60, 23, 23]          32,400\n",
      "     BatchNorm2d-123           [-1, 60, 23, 23]             120\n",
      "       LeakyReLU-124           [-1, 60, 23, 23]               0\n",
      "          Conv2d-125          [-1, 240, 23, 23]          14,400\n",
      "     BatchNorm2d-126          [-1, 240, 23, 23]             480\n",
      "       LeakyReLU-127          [-1, 240, 23, 23]               0\n",
      "      Bottleneck-128          [-1, 240, 23, 23]               0\n",
      "          Conv2d-129           [-1, 60, 23, 23]          14,400\n",
      "     BatchNorm2d-130           [-1, 60, 23, 23]             120\n",
      "       LeakyReLU-131           [-1, 60, 23, 23]               0\n",
      "          Conv2d-132           [-1, 60, 23, 23]          32,400\n",
      "     BatchNorm2d-133           [-1, 60, 23, 23]             120\n",
      "       LeakyReLU-134           [-1, 60, 23, 23]               0\n",
      "          Conv2d-135          [-1, 240, 23, 23]          14,400\n",
      "     BatchNorm2d-136          [-1, 240, 23, 23]             480\n",
      "       LeakyReLU-137          [-1, 240, 23, 23]               0\n",
      "      Bottleneck-138          [-1, 240, 23, 23]               0\n",
      "          Conv2d-139           [-1, 96, 23, 23]          23,040\n",
      "     BatchNorm2d-140           [-1, 96, 23, 23]             192\n",
      "       LeakyReLU-141           [-1, 96, 23, 23]               0\n",
      "          Conv2d-142           [-1, 96, 12, 12]          82,944\n",
      "     BatchNorm2d-143           [-1, 96, 12, 12]             192\n",
      "       LeakyReLU-144           [-1, 96, 12, 12]               0\n",
      "          Conv2d-145          [-1, 384, 12, 12]          36,864\n",
      "     BatchNorm2d-146          [-1, 384, 12, 12]             768\n",
      "          Conv2d-147          [-1, 384, 12, 12]          92,160\n",
      "     BatchNorm2d-148          [-1, 384, 12, 12]             768\n",
      "       LeakyReLU-149          [-1, 384, 12, 12]               0\n",
      "      Bottleneck-150          [-1, 384, 12, 12]               0\n",
      "          Conv2d-151           [-1, 96, 12, 12]          36,864\n",
      "     BatchNorm2d-152           [-1, 96, 12, 12]             192\n",
      "       LeakyReLU-153           [-1, 96, 12, 12]               0\n",
      "          Conv2d-154           [-1, 96, 12, 12]          82,944\n",
      "     BatchNorm2d-155           [-1, 96, 12, 12]             192\n",
      "       LeakyReLU-156           [-1, 96, 12, 12]               0\n",
      "          Conv2d-157          [-1, 384, 12, 12]          36,864\n",
      "     BatchNorm2d-158          [-1, 384, 12, 12]             768\n",
      "       LeakyReLU-159          [-1, 384, 12, 12]               0\n",
      "      Bottleneck-160          [-1, 384, 12, 12]               0\n",
      "          Conv2d-161           [-1, 96, 12, 12]          36,864\n",
      "     BatchNorm2d-162           [-1, 96, 12, 12]             192\n",
      "       LeakyReLU-163           [-1, 96, 12, 12]               0\n",
      "          Conv2d-164           [-1, 96, 12, 12]          82,944\n",
      "     BatchNorm2d-165           [-1, 96, 12, 12]             192\n",
      "       LeakyReLU-166           [-1, 96, 12, 12]               0\n",
      "          Conv2d-167          [-1, 384, 12, 12]          36,864\n",
      "     BatchNorm2d-168          [-1, 384, 12, 12]             768\n",
      "       LeakyReLU-169          [-1, 384, 12, 12]               0\n",
      "      Bottleneck-170          [-1, 384, 12, 12]               0\n",
      "          Conv2d-171           [-1, 96, 12, 12]          36,864\n",
      "     BatchNorm2d-172           [-1, 96, 12, 12]             192\n",
      "       LeakyReLU-173           [-1, 96, 12, 12]               0\n",
      "          Conv2d-174           [-1, 96, 12, 12]          82,944\n",
      "     BatchNorm2d-175           [-1, 96, 12, 12]             192\n",
      "       LeakyReLU-176           [-1, 96, 12, 12]               0\n",
      "          Conv2d-177          [-1, 384, 12, 12]          36,864\n",
      "     BatchNorm2d-178          [-1, 384, 12, 12]             768\n",
      "       LeakyReLU-179          [-1, 384, 12, 12]               0\n",
      "      Bottleneck-180          [-1, 384, 12, 12]               0\n",
      "          Conv2d-181           [-1, 96, 12, 12]          36,864\n",
      "     BatchNorm2d-182           [-1, 96, 12, 12]             192\n",
      "       LeakyReLU-183           [-1, 96, 12, 12]               0\n",
      "          Conv2d-184           [-1, 96, 12, 12]          82,944\n",
      "     BatchNorm2d-185           [-1, 96, 12, 12]             192\n",
      "       LeakyReLU-186           [-1, 96, 12, 12]               0\n",
      "          Conv2d-187          [-1, 384, 12, 12]          36,864\n",
      "     BatchNorm2d-188          [-1, 384, 12, 12]             768\n",
      "       LeakyReLU-189          [-1, 384, 12, 12]               0\n",
      "      Bottleneck-190          [-1, 384, 12, 12]               0\n",
      "          Conv2d-191           [-1, 96, 12, 12]          36,864\n",
      "     BatchNorm2d-192           [-1, 96, 12, 12]             192\n",
      "       LeakyReLU-193           [-1, 96, 12, 12]               0\n",
      "          Conv2d-194           [-1, 96, 12, 12]          82,944\n",
      "     BatchNorm2d-195           [-1, 96, 12, 12]             192\n",
      "       LeakyReLU-196           [-1, 96, 12, 12]               0\n",
      "          Conv2d-197          [-1, 384, 12, 12]          36,864\n",
      "     BatchNorm2d-198          [-1, 384, 12, 12]             768\n",
      "       LeakyReLU-199          [-1, 384, 12, 12]               0\n",
      "      Bottleneck-200          [-1, 384, 12, 12]               0\n",
      "          Conv2d-201           [-1, 96, 12, 12]          36,864\n",
      "     BatchNorm2d-202           [-1, 96, 12, 12]             192\n",
      "       LeakyReLU-203           [-1, 96, 12, 12]               0\n",
      "          Conv2d-204           [-1, 96, 12, 12]          82,944\n",
      "     BatchNorm2d-205           [-1, 96, 12, 12]             192\n",
      "       LeakyReLU-206           [-1, 96, 12, 12]               0\n",
      "          Conv2d-207          [-1, 384, 12, 12]          36,864\n",
      "     BatchNorm2d-208          [-1, 384, 12, 12]             768\n",
      "       LeakyReLU-209          [-1, 384, 12, 12]               0\n",
      "      Bottleneck-210          [-1, 384, 12, 12]               0\n",
      "          Conv2d-211           [-1, 96, 12, 12]          36,864\n",
      "     BatchNorm2d-212           [-1, 96, 12, 12]             192\n",
      "       LeakyReLU-213           [-1, 96, 12, 12]               0\n",
      "          Conv2d-214           [-1, 96, 12, 12]          82,944\n",
      "     BatchNorm2d-215           [-1, 96, 12, 12]             192\n",
      "       LeakyReLU-216           [-1, 96, 12, 12]               0\n",
      "          Conv2d-217          [-1, 384, 12, 12]          36,864\n",
      "     BatchNorm2d-218          [-1, 384, 12, 12]             768\n",
      "       LeakyReLU-219          [-1, 384, 12, 12]               0\n",
      "      Bottleneck-220          [-1, 384, 12, 12]               0\n",
      "AdaptiveAvgPool2d-221            [-1, 384, 1, 1]               0\n",
      "          Linear-222                   [-1, 10]           3,850\n",
      "================================================================\n",
      "Total params: 1,996,986\n",
      "Trainable params: 1,996,986\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.11\n",
      "Forward/backward pass size (MB): 132.52\n",
      "Params size (MB): 7.62\n",
      "Estimated Total Size (MB): 140.24\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model,(3,96,96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, 10, T_mult=2, eta_min=0, last_epoch=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f68f54d1f60>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr_list=[]\n",
    "lr=0.001\n",
    "for _ in range(300):\n",
    "    scheduler.step()\n",
    "    lr_list.append(optimizer.param_groups[0]['lr'])\n",
    "plt.plot(lr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, 10, T_mult=2, eta_min=0, last_epoch=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_path=\"/home/jupyter-deeplearning/res_model/\"\n",
    "trn_loss_list = []\n",
    "val_loss_list = []\n",
    "val_acc_list = []\n",
    "total_epoch=300\n",
    "model_char=\"3.0\"\n",
    "model_name=\"\"\n",
    "patience=10\n",
    "start_early_stop_check=0\n",
    "saving_start_epoch=10\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    trn_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        if torch.cuda.is_available():\n",
    "            inputs=inputs.cuda()\n",
    "            labels=labels.cuda()\n",
    "        # grad init\n",
    "        optimizer.zero_grad()\n",
    "        # forward propagation\n",
    "        output= model(inputs)\n",
    "        # calculate loss\n",
    "        loss=criterion(output, labels)\n",
    "        # back propagation \n",
    "        loss.backward()\n",
    "        # weight update\n",
    "        optimizer.step()\n",
    "        \n",
    "        # trn_loss summary\n",
    "        trn_loss += loss.item()\n",
    "        # del (memory issue)\n",
    "        del loss\n",
    "        del output\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        cor_match = 0\n",
    "        for j, val in enumerate(test_loader):\n",
    "            val_x, val_label = val\n",
    "            if torch.cuda.is_available():\n",
    "                val_x = val_x.cuda()\n",
    "                val_label =val_label.cuda()\n",
    "            val_output = model(val_x)\n",
    "            v_loss = criterion(val_output, val_label)\n",
    "            val_loss += v_loss\n",
    "            _, predicted=torch.max(val_output,1)\n",
    "            cor_match+=np.count_nonzero(predicted.cpu().detach()==val_label.cpu().detach())\n",
    "    del val_output\n",
    "    del v_loss\n",
    "    del predicted\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    \n",
    "    \n",
    "    trn_loss_list.append(trn_loss/len(train_loader))\n",
    "    val_loss_list.append(val_loss/len(test_loader))\n",
    "    val_acc=cor_match/(len(test_loader)*batch_size)\n",
    "    val_acc_list.append(val_acc)\n",
    "    now = time.localtime()\n",
    "    print (\"%04d/%02d/%02d %02d:%02d:%02d\" % (now.tm_year, now.tm_mon, now.tm_mday, now.tm_hour, now.tm_min, now.tm_sec))\n",
    "\n",
    "    print(\"epoch: {}/{} | trn loss: {:.4f} | val loss: {:.4f} | val accuracy: {:.4f}% \\n\".format(\n",
    "                epoch+1, total_epoch, trn_loss / len(train_loader), val_loss / len(test_loader), val_acc*100\n",
    "            ))\n",
    "    \n",
    "    \n",
    "    if epoch+1>2:\n",
    "        if val_loss_list[-1]>val_loss_list[-2]:\n",
    "            start_early_stop_check=1\n",
    "    else:\n",
    "        val_loss_min=val_loss_list[-1]\n",
    "        \n",
    "    if start_early_stop_check:\n",
    "        early_stop_temp=val_loss_list[-patience:]\n",
    "        if all(early_stop_temp[i]<early_stop_temp[i+1] for i in range (len(early_stop_temp)-1)):\n",
    "            print(\"Early stop!\")\n",
    "            break\n",
    "            \n",
    "    if epoch+1>saving_start_epoch:\n",
    "        if val_loss_list[-1]<val_loss_min:\n",
    "            if os.path.isfile(model_name):\n",
    "                os.remove(model_name)\n",
    "            val_loss_min=val_loss_list[-1]\n",
    "            model_name=saving_path+\"Custom_model_\"+model_char+\"_{:.3f}\".format(val_loss_min)\n",
    "            torch.save(model, model_name)\n",
    "            print(\"Model replaced and saved as \",model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "dataset = datasets.ImageFolder(\n",
    "        train_path_path,\n",
    "        transforms.Compose([\n",
    "        transforms.RandomRotation(degrees=45),\n",
    "        transforms.Resize(120),    \n",
    "        transforms.ColorJitter(.3,.3,.3,.3),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "for _ in range (19):\n",
    "    aug_data = datasets.ImageFolder(\n",
    "        train_path_path,\n",
    "        transforms.Compose([\n",
    "        transforms.RandomRotation(degrees=45),\n",
    "        transforms.Resize(120),    \n",
    "        transforms.ColorJitter(.3,.3,.3,.3),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "    dataset=torch.utils.data.ConcatDataset([dataset,aug_data])\n",
    "valset = datasets.ImageFolder(\n",
    "        test_path_path,\n",
    "        transforms.Compose([\n",
    "        transforms.RandomRotation(degrees=45),\n",
    "        transforms.Resize(120),\n",
    "        transforms.ColorJitter(.3,.3,.3,.3),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "for _ in range (19):\n",
    "    aug_data = datasets.ImageFolder(\n",
    "        test_path_path,\n",
    "        transforms.Compose([\n",
    "        transforms.RandomRotation(degrees=45),\n",
    "        transforms.Resize(120),    \n",
    "        transforms.ColorJitter(.3,.3,.3,.3),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "    valset=torch.utils.data.ConcatDataset([valset,aug_data])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=True,\n",
    "        num_workers=8, pin_memory=True,drop_last=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        valset, batch_size=batch_size, shuffle=True,\n",
    "        num_workers=8, pin_memory=True,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, 10, T_mult=2, eta_min=0, last_epoch=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fine tuning\n",
    "\n",
    "total_epoch=140\n",
    "model_char=\"3.0\"\n",
    "model_name=\"\"\n",
    "patience=10\n",
    "start_early_stop_check=0\n",
    "saving_start_epoch=10\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    trn_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        if torch.cuda.is_available():\n",
    "            inputs=inputs.cuda()\n",
    "            labels=labels.cuda()\n",
    "        # grad init\n",
    "        optimizer.zero_grad()\n",
    "        # forward propagation\n",
    "        output= model(inputs)\n",
    "        # calculate loss\n",
    "        loss=criterion(output, labels)\n",
    "        # back propagation \n",
    "        loss.backward()\n",
    "        # weight update\n",
    "        optimizer.step()\n",
    "        \n",
    "        # trn_loss summary\n",
    "        trn_loss += loss.item()\n",
    "        # del (memory issue)\n",
    "        del loss\n",
    "        del output\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        cor_match = 0\n",
    "        for j, val in enumerate(test_loader):\n",
    "            val_x, val_label = val\n",
    "            if torch.cuda.is_available():\n",
    "                val_x = val_x.cuda()\n",
    "                val_label =val_label.cuda()\n",
    "            val_output = model(val_x)\n",
    "            v_loss = criterion(val_output, val_label)\n",
    "            val_loss += v_loss\n",
    "            _, predicted=torch.max(val_output,1)\n",
    "            cor_match+=np.count_nonzero(predicted.cpu().detach()==val_label.cpu().detach())\n",
    "    del val_output\n",
    "    del v_loss\n",
    "    del predicted\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    \n",
    "    \n",
    "    trn_loss_list.append(trn_loss/len(train_loader))\n",
    "    val_loss_list.append(val_loss/len(test_loader))\n",
    "    val_acc=cor_match/(len(test_loader)*batch_size)\n",
    "    val_acc_list.append(val_acc)\n",
    "    now = time.localtime()\n",
    "    print (\"%04d/%02d/%02d %02d:%02d:%02d\" % (now.tm_year, now.tm_mon, now.tm_mday, now.tm_hour, now.tm_min, now.tm_sec))\n",
    "\n",
    "    print(\"epoch: {}/{} | trn loss: {:.4f} | val loss: {:.4f} | val accuracy: {:.4f}% \\n\".format(\n",
    "                epoch+1, total_epoch, trn_loss / len(train_loader), val_loss / len(test_loader), val_acc*100\n",
    "            ))\n",
    "    \n",
    "    \n",
    "    if epoch+1>2:\n",
    "        if val_loss_list[-1]>val_loss_list[-2]:\n",
    "            start_early_stop_check=1\n",
    "    else:\n",
    "        val_loss_min=val_loss_list[-1]\n",
    "        \n",
    "    if start_early_stop_check:\n",
    "        early_stop_temp=val_loss_list[-patience:]\n",
    "        if all(early_stop_temp[i]<early_stop_temp[i+1] for i in range (len(early_stop_temp)-1)):\n",
    "            print(\"Early stop!\")\n",
    "            break\n",
    "            \n",
    "    if epoch+1>saving_start_epoch:\n",
    "        if val_loss_list[-1]<val_loss_min:\n",
    "            if os.path.isfile(model_name):\n",
    "                os.remove(model_name)\n",
    "            val_loss_min=val_loss_list[-1]\n",
    "            model_name=\"Custom_model_\"+model_char+\"_{:.3f}\".format(val_loss_min)\n",
    "            torch.save(model, model_name)\n",
    "            print(\"Model replaced and saved as \",model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plt.ylabel(\"val_accuracy\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(val_acc_list)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_acc_list=np.array(val_acc_list)\n",
    "# np.savetxt(\"ver_2.0.txt\", val_acc_list, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import pandas as pd\n",
    "# import argparse\n",
    "# import time\n",
    "# import torchvision\n",
    "# import torchvision.transforms as transforms\n",
    "# from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize = transforms.Normalize(mean=[0.507, 0.487, 0.441],\n",
    "#                                      std=[0.267, 0.256, 0.276])\n",
    "# test_transform = transforms.Compose([\n",
    "#         transforms.ToTensor(),\n",
    "#         normalize\n",
    "#     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset = torchvision.datasets.ImageFolder('./data/test', transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category = []\n",
    "# for input, _ in test_loader:\n",
    "#     input = input.cuda()\n",
    "#     output = model(input)\n",
    "#     output = torch.argmax(output, dim=1)\n",
    "#     Category = Category + output.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Id = list(range(0, 8000))\n",
    "# samples = {\n",
    "#    'Id': Id,\n",
    "#    'Category': Category \n",
    "# }\n",
    "# df = pd.DataFrame(samples, columns=['Id', 'Category'])\n",
    "\n",
    "# df.to_csv('submission_2.0_2.csv', index=False)\n",
    "# print('Done!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
